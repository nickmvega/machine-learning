{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a118c506",
   "metadata": {},
   "source": [
    "# Project 2: Classification with Kernelized Perceptron\n",
    "\n",
    "## Objectives\n",
    "Your goal in this project is to get comfortable in implementing kernelized perceptron for classification. To complete this project, you should understand the following:\n",
    "\n",
    "* How to use basic math and machine learning modules in python such as numpy, matplotlib, and sklearn\n",
    "* How to train a kernel perceoptron model *from scratch*\n",
    "* How to select an approprite kernel function for a task.\n",
    "* How to perform model section when facing multiple choices\n",
    "* How to evaluate the test results and visualize the outcome of an ML model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfedf32f",
   "metadata": {},
   "source": [
    "## Deliverable\n",
    "* Project report/writeup: A `project2_report_lastname.pdf` file with corresponding plots and results for the project. Follow the `Project 2 - Report (Individual Submission)` link on Gradescope to upload this file. The project report should include a brief justification of your solution at a high-level, e.g., using any relevant explanations, equations, or pictures that help to explain your solution. You should also describe what your code does, e.g. using a couple of sentences per function to describe your code structure. \n",
    "\n",
    "* Source code: A `project2_src_lastname1[_lastname2].ipynb` (or `.zip`) file with a working copy of your solutions compiled in a Jupyter notebook. Follow the `Project 2 - Source Code (Group Submission)` link to upload this file.\n",
    "\n",
    "\n",
    "## Logistics\n",
    "\n",
    "* You can work in groups of 1-2 students for each course project, and it's your responsibility to find a group (e.g. use Ed Discussion). \n",
    "* Every member of a group must complete and submit the project report/writeup individually. While the source code can be the same for all group members, the project report needs to be written independently by each person and, thus, should differ among team member and students more generally.\n",
    "* One one group member need to submit the source code. If you submit as a group, make sure to include your teammate in the group submission. Instructions for team submission can be found [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "* Grades will be provided based on the individual project report. The source code submission will not be graded, but the teaching staff may check the source files if they see the need for reproducing your results when going through your project report. \n",
    "* Failure to submit the source code will lead to a deduction of points from your total.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9303c50",
   "metadata": {},
   "source": [
    "# Task 2A (60pts)\n",
    "In this problem, you will use perceptron to deal with a 2-D classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e864b1",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "The reference code for generating the training and test set (and plot) is provided as below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d04a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(40)\n",
    "\n",
    "# Step 1: Define dataset parameters\n",
    "num_samples = 100\n",
    "dimensionality = 2\n",
    "num_classes = 2\n",
    "\n",
    "# Step 2: Generate random data points for each class\n",
    "mean_class1 = np.random.randn(dimensionality) * 2\n",
    "mean_class2 = np.random.randn(dimensionality) * 2 + 5\n",
    "data_class1 = mean_class1 + np.random.randn(num_samples // 2, dimensionality)\n",
    "data_class2 = mean_class2 + np.random.randn(num_samples // 2, dimensionality)\n",
    "\n",
    "# Step 3: Assign class labels (-1 for class 1 and 1 for class 2)\n",
    "labels_class1 = -np.ones(num_samples // 2)\n",
    "labels_class2 = np.ones(num_samples // 2)\n",
    "\n",
    "# Step 4: Combine data and labels, and shuffle the dataset\n",
    "data = np.vstack((data_class1, data_class2))\n",
    "labels = np.hstack((labels_class1, labels_class2))\n",
    "permutation = np.random.permutation(num_samples)\n",
    "data = data[permutation]\n",
    "labels = labels[permutation]\n",
    "\n",
    "# Split the dataset into training and testing sets (e.g., 80% training, 20% testing)\n",
    "split_ratio = 0.8\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(data, labels, test_size=1-split_ratio, random_state=42)\n",
    "\n",
    "# Visualize the dataset (optional)\n",
    "plt.scatter(data_class1[:, 0], data_class1[:, 1], label='Class 1 (negative)', marker='o')\n",
    "plt.scatter(data_class2[:, 0], data_class2[:, 1], label='Class 2 (positive)', marker='x')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8736474b",
   "metadata": {},
   "source": [
    "## Task 2A.1. Perceptron (25pts)\n",
    "\n",
    "In this section, we are going to implement a binary classifier with the `Perceptron` class. \n",
    "\n",
    "<span style=\"color:red\"> TODO: </span> Fill in the code in the `Perceptron` class to implement the `fit`, `project`, and `predict` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c53f6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    def __init__(self, T=1):\n",
    "        self.T = T # number of iterations\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train perceptron model on data X with labels y and iteration T.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features, dtype=np.float64)\n",
    "        self.b = 0.0\n",
    "        #### TODO: YOUR CODE HERE ####\n",
    "        for _ in range(self.T):\n",
    "            for i in range(n_samples):\n",
    "                if self.predict(X[i])[0] != y[i]:\n",
    "                    self.w += y[i] * X[i]\n",
    "                    self.b += y[i]        \n",
    "\n",
    "    def project(self, X):\n",
    "        \"\"\"\n",
    "        Project data X onto the learned hyperplane with weights w and bias b.\n",
    "        \"\"\"\n",
    "        #### TODO: YOUR CODE HERE ####\n",
    "        return \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X. Must use the project method.\n",
    "        \"\"\"\n",
    "        X = np.atleast_2d(X)\n",
    "\n",
    "        #### TODO: YOUR CODE HERE ####\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ad25f",
   "metadata": {},
   "source": [
    "Then we evaluate its accuracy and plot the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "46077362",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceptron(T=5)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypred = model.predict(Xtest)\n",
    "print('Accuracy: %.2f%%' % (np.mean(ypred == ytest) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42666ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "w = model.w\n",
    "b = model.b\n",
    "x1 = np.linspace(-5, 10, 100)\n",
    "x2 = (-w[0] * x1 - b) / w[1]\n",
    "plt.plot(x1, x2, 'k-')\n",
    "plt.scatter(data_class1[:, 0], data_class1[:, 1], label='Class 1 (negative)', marker='o')\n",
    "plt.scatter(data_class2[:, 0], data_class2[:, 1], label='Class 2 (positive)', marker='x')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fea94f9",
   "metadata": {},
   "source": [
    "## Task 2A.2. Kernel Trick (35pts)\n",
    "\n",
    "Recall that in class (\"Kernel Methods\"), we discussed the *Kernel Perceptron* algorithm. The decision function for the Kernel Perceptron is given by\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\text{sign}\\left(\\sum_{i=1}^{n} \\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{x})\\right)\n",
    "$$\n",
    "\n",
    "where $k(\\mathbf{x}_i, \\mathbf{x})$ is the kernel function, $\\alpha_i$ are the learned weights, and $y_i$ are the labels.\n",
    "\n",
    "The kernel (Gram) matrix induced by kernel function *k* over *n* data points is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{K}=\n",
    "\\left(\\begin{array}{ccc} \n",
    "k(\\mathbf{x}_1,\\mathbf{x}_1) & \\dots & k(\\mathbf{x}_1,\\mathbf{x}_n)\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "k(\\mathbf{x}_n,\\mathbf{x}_1) & \\dots & k(\\mathbf{x}_n,\\mathbf{x}_n)\n",
    "\\end{array}\\right)\n",
    "$$ \n",
    "\n",
    "Given a test data point **x**, the predicted label is\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{sign}\\left(\\sum_{i=1}^{n} \\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{x})\\right)\n",
    "$$\n",
    "\n",
    "The Kernel Perceptron algorithm iteratively updates the weights $\\alpha_i$ based on the misclassified points in the training set (please refer to lecture notes on \"kernel\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of samples per quadrant\n",
    "n = 50\n",
    "\n",
    "# Standard deviation for Gaussian distribution\n",
    "std_dev = 0.1\n",
    "\n",
    "# Quadrant 1 and 3: label 1\n",
    "q1 = np.random.normal(loc=[0.25, 0.25], scale=std_dev, size=(n, 2))\n",
    "q3 = np.random.normal(loc=[0.75, 0.75], scale=std_dev, size=(n, 2))\n",
    "\n",
    "# Quadrant 2 and 4: label -1\n",
    "q2 = np.random.normal(loc=[0.75, 0.25], scale=std_dev, size=(n, 2))\n",
    "q4 = np.random.normal(loc=[0.25, 0.75], scale=std_dev, size=(n, 2))\n",
    "\n",
    "X = np.vstack((q1, q3, q2, q4))\n",
    "y = np.hstack((np.ones(2*n), -np.ones(2*n)))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7026f2c1",
   "metadata": {},
   "source": [
    "The code below plots the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6b266efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "def plot_decision_bounday_kernel_perceptron(model, ax=None):\n",
    "    x1 = np.linspace(0, 1, 50)\n",
    "    x2 = np.linspace(0, 1, 50)\n",
    "    xx1, xx2 = np.meshgrid(x1, x2)\n",
    "    Z = np.zeros(xx1.shape)\n",
    "    for i in range(xx1.shape[0]):\n",
    "        for j in range(xx1.shape[1]):\n",
    "            Z[i,j] = model.predict([xx1[i,j], xx2[i,j]])\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the decision boundary and the data\n",
    "    contour = ax.contourf(xx1, xx2, Z, alpha=0.4)\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k')\n",
    "\n",
    "    return contour, scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe9f23",
   "metadata": {},
   "source": [
    "Recall in lecture, we have seen different kernels for $\\mathbb{R}^d$. Here are three common kernels below:\n",
    "\n",
    "\\begin{align*}\n",
    "k_{\\text{poly}}(\\mathbf{x},\\mathbf{x}', d)&=(1+\\mathbf{x}^\\top \\mathbf{x}')^d.\\\\\n",
    "k_{\\text{RBF}}(\\mathbf{x},\\mathbf{x}', \\sigma) &= \\exp(-\\frac{\\lVert \\mathbf{x}-\\mathbf{x'} \\rVert^2_2}{2\\sigma^2})\\\\\n",
    "k_{\\text{laplace}}(\\mathbf{x},\\mathbf{x}', \\sigma) &= \\exp(-\\frac{\\lVert \\mathbf{x}-\\mathbf{x'} \\rVert _1}{\\sigma})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bf1cc7",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> TODO: </span> Fill the following code block for kernel functions and kernel perceptron. (You should *not* use the other libraries such as scikit-learn.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b5f88ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialKernel:\n",
    "    def __init__(self, p=1):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        #### TODO: YOUR CODE HERE\n",
    "        return \n",
    "\n",
    "class GaussianKernel:\n",
    "    def __init__(self, sigma=5):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        #### TODO: YOUR CODE HERE\n",
    "        return \n",
    "    \n",
    "class LaplaceKernel:\n",
    "    def __init__(self, sigma=5):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        #### TODO: YOUR CODE HERE\n",
    "        return \n",
    "    \n",
    "class KernelPerceptron(object):\n",
    "    def __init__(self, kernel=PolynomialKernel(p = 1), T=1):\n",
    "        self.kernel = kernel\n",
    "        self.T = T # number of iterations\n",
    "        self.alpha = None\n",
    "        self.Xtrain = None\n",
    "        self.ytrain = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.Xtrain, self.ytrain = X, y\n",
    "        n_samples, n_features = X.shape\n",
    "        self.alpha = np.zeros(n_samples, dtype=np.float64)\n",
    "        # Gram matrix\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        #### TODO: YOUR CODE HERE\n",
    "        \n",
    "        \n",
    "    def project(self, X):\n",
    "        #### TODO: YOUR CODE HERE\n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.atleast_2d(X)\n",
    "        #### TODO: YOUR CODE HERE\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803cf12",
   "metadata": {},
   "source": [
    "### Is the data linearly separable?\n",
    "Try the linear kernel (polynomial with $p = 1$) and plot the decision boundary.\n",
    "\n",
    "<span style=\"color:red\"> TODO: </span> Report on the performance and behavior of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e10ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## results for linear kernel\n",
    "model = KernelPerceptron(kernel=PolynomialKernel(p = 1), T=10)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypred = model.predict(Xtest)\n",
    "print('Accuracy: %.2f%%' % (accuracy_score(ytest, ypred) * 100))\n",
    "plot_decision_bounday_kernel_perceptron(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee2ade",
   "metadata": {},
   "source": [
    "### Can we do better with more powerful kernels?\n",
    "<span style=\"color:red\"> TODO: </span> Perform regression with polynomial, Gaussian, and Laplace kernels with different parameters, and visualize the accuracy as the parameter changes. \n",
    "\n",
    "Report on their decision boundaries, accuracy, and how the number of epochs required to reach a plateau in accuracy. Analyze what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f89873",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: YOUR CODE HERE\n",
    "# perform regression with polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a97a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: YOUR CODE HERE\n",
    "# perform regression with Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c425bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: YOUR CODE HERE\n",
    "# perform regression with Laplace kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a6462",
   "metadata": {},
   "source": [
    "# Task 2B: Real-World Data Analysis: Seoul Bike Rental Data (40pts)\n",
    "\n",
    "In this task, we will analyze the `SeoulBikeData.csv` dataset, which provides information about bike rentals in Seoul. The dataset includes:\n",
    "- **6 Features**: Weather-related conditions like temperature, humidity, and wind speed.\n",
    "- **1 Time Feature**: Hour of the day.\n",
    "- **Target**: The number of rented bikes, with the objective of predicting whether `Rented Bike Count > 500`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d2937",
   "metadata": {},
   "source": [
    "## Steps to Complete:\n",
    "1. **Load and Explore the Dataset**:\n",
    "   - Load the `SeoulBikeData.csv` file using `pandas`.\n",
    "   - Display descriptive statistics and visualize feature distributions (e.g., histograms, pair plots).\n",
    "\n",
    "2. **Preprocessing**:\n",
    "   - Convert `Rented Bike Count` into a binary target (`1` if > 500, else `0`).\n",
    "   - Normalize the numerical features using min-max scaling or standardization.\n",
    "\n",
    "3. **Kernel-Based Modeling**:\n",
    "     - **Polynomial Kernel**: $k_{\\text{poly}}(\\mathbf{x}, \\mathbf{x}') = (1 + \\mathbf{x}^\\top \\mathbf{x}')^d$\n",
    "     - **Gaussian Kernel (RBF)**: $k_{\\text{RBF}}(\\mathbf{x}, \\mathbf{x}') = \\exp\\left(-\\frac{\\lVert \\mathbf{x} - \\mathbf{x}' \\rVert_2^2}{2 \\sigma^2}\\right)$\n",
    "     \n",
    "   - Optimize $\\sigma$ for the Laplace kernel during training using cross-validation.\n",
    "\n",
    "4. **Evaluation and Analysis**:\n",
    "   - Compare the performance of different kernels using accuracy and\n",
    "   classification reports.\n",
    "   - Visualize decision boundaries for the Laplace kernel with the optimal $\\sigma$ using principle component analysis (PCA) to project data in higher dimensional feature space to 3 dimensions. Then plot the decision boundary in the same graph to visualize. Assuming the features are linearly independent, it would take the full dimension of the feature to capture 100 percent of the variation, however if we assume the data is of low \"numerical rank\", plotting the first say 3 dominant dimension of the feature will give a good representation of the data.\n",
    "   - Use `pca_3d = PCA(n_components=3); X_pca_3d = pca_3d.fit_transform(Xtest)` followed by `pca_3d.explained_variance_ratio_` and `np.sum(explained_variance_ratio) * 100:.2f}%` to see how much of the variance is explained by 3 dominant principle component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3fc922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the data to the current directory. Change the path or install unzip if needed.\n",
    "# You may use your OS tool or the following code.\n",
    "# You can safely comment out this line once the data is unzipped.\n",
    "!unzip ./pa2_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c7cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "filename = 'data/SeoulBikeData.csv'\n",
    "df = pd.read_csv(filename).drop(['Date', 'Seasons', 'Holiday', 'Functioning Day'], axis=1)\n",
    "df = df.sample(n=1000, random_state=4)\n",
    "X = df.drop(['Rented Bike Count'],  axis=1)[[\n",
    "                                            'Hour', \n",
    "                                            'Temperature (deg C)', \n",
    "                                            'Humidity(%)', \n",
    "                                            'Visibility (10m)',\n",
    "                                            'Dew point temperature (deg C)',\n",
    "                                            'Solar Radiation (MJ/m2)',\n",
    "                                            'Rainfall(mm)'\n",
    "                                            ]]\n",
    "y = df['Rented Bike Count'].values\n",
    "# binarize y\n",
    "y = np.where(y <= 500, -1, 1)\n",
    "\n",
    "print(f\"Shape X {X.shape}\")\n",
    "print(f\"Shape y {y.shape}\")\n",
    "print(\"y distribution: \", np.unique(y, return_counts=True))\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "Xtrain, Xval = Xtrain[:int(len(Xtrain)*0.8)], Xtrain[int(len(Xtrain)*0.8):]\n",
    "ytrain, yval = ytrain[:int(len(ytrain)*0.8)], ytrain[int(len(ytrain)*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4611565f",
   "metadata": {},
   "source": [
    "### Kernel Selection ###\n",
    "\n",
    "<span style=\"color:red\"> TODO: </span> Find a proper kernel function to solve the classification task. \n",
    "\n",
    "1. Identify and implement a new kernel function of your choice, e.g., sigmoid kernel. \n",
    "\n",
    "2. Use two other kernels used in 2A, namely `PolynomialKernel` and `GaussianKernel`. \n",
    "\n",
    "3. Try to optimize hyperparameter(s) of each kernel. Please do a bit of background research and select a range of 5 values for the parameters that seem reasonable and create an additional `val` split from the training data to select the optimal hyperparameter. Then train a model using the best hyperparameter(s) on the whole training set and test the model on the test set. Justify in your report your choice for the 5 values. \n",
    "\n",
    "Discuss how accuracy compares across all methods. \n",
    "\n",
    "Discuss the difference between the accuracy for data you use to train the model and data the model has not seen. (i.e study the generalization error for each model). What is overfitting in this context?\n",
    "\n",
    "*Note: Depending on your implementation, this may take several minutes to run. If you find that it is taking prohibitively long, try to optimize your code. A reasonable accuracy can range from 60% to 80% depending on the selected kernel.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1fc30126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: YOUR CODE HERE\n",
    "class NewKernel(object):\n",
    "    def __init__(self):\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cebc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best hyperparameter for a kernel from Task 2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54095e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best hyperparameter for another kernel from Task 2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc1267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best hyperparameter for your new kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data on projected PCA axis in 3d, \n",
    "# show much how variance is explained by 3 dominant principal components\n",
    "# (see evaluation and analysis for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eef430",
   "metadata": {},
   "source": [
    "# Task 2C (Bonus) Feature Selection (20 pts)\n",
    "\n",
    "In this task, we will implement and analyze 2 feature selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea8f885",
   "metadata": {},
   "source": [
    "## Task 2C.1. Greedy forward selection (10pts)\n",
    "The full dataset is re-imported below. Note the features of `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b847a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "filename = 'data/SeoulBikeData.csv'\n",
    "df = pd.read_csv(filename).drop(['Date', 'Seasons', 'Holiday', 'Functioning Day'], axis=1)\n",
    "df = df.sample(n=1000, random_state=4)\n",
    "X = df.drop(['Rented Bike Count'],  axis=1)\n",
    "display(df.head())\n",
    "feature_names = X.columns\n",
    "y = df['Rented Bike Count'].values\n",
    "# binarize y\n",
    "y = np.where(y <= 500, -1, 1)\n",
    "\n",
    "print(f\"Shape X {X.shape}\")\n",
    "print(f\"Shape y {y.shape}\")\n",
    "print(\"y distribution: \", np.unique(y, return_counts=True))\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e399a7",
   "metadata": {},
   "source": [
    " Recall the greedy backward selection algorithm in class. The version you will implement below should use an averaged cross-validated accuracy.\n",
    "\n",
    "The Greedy forward feature selection algorithm (using accuracy):\n",
    "\n",
    "Initialize $S = \\emptyset, A_0 = -1$. Then for $i = 1, \\dots, d$, find the best element to add $$s_i = \\arg\\max_{j \\in S} {A_{cv}(S \\cup \\set{j})}$$\n",
    "with the corresponding maximum accuracy $A_i = A_{cv}(S \\cup \\set{s_i})$. If $A_{i+1} < A_i$, break, else set $S = S \\cup \\set{s_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d68d2c0",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> TODO: </span> Finish the implementation of cross validation using accuracy as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e8897814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_cross_validation(Xtrain, ytrain, model, k = 10):\n",
    "    m = len(ytrain)\n",
    "    fold_size = m//k\n",
    "    accuracy_list = []\n",
    "    for i in range(k):\n",
    "        tr_idx = list(range(i*fold_size)) + list(range((i+1) * fold_size, m))\n",
    "        val_idx = list(range(i * fold_size, (i+1) * fold_size))\n",
    "        #### TODO: YOUR CODE HERE\n",
    "        \n",
    "    return np.mean(accuracy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec2146a",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> TODO: </span> Implement the greedy forward feature selection algorithm. Your output should contain a list of indices, corresponding to the chosen features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d1370dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_forward(Xtrain, ytrain, k = 5):\n",
    "    d = Xtrain.shape[1]\n",
    "    S = []\n",
    "    V = list(range(d))\n",
    "    best_acc = -1\n",
    "    best_acc_list = []\n",
    "    for _ in range(d):\n",
    "        #### TODO: YOUR CODE HERE\n",
    "        pass\n",
    "        \n",
    "    return S, best_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f1592",
   "metadata": {},
   "source": [
    "The code below plots cross validated accuracy against the number of features added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "S, best_acc_list = greedy_forward(Xtrain, ytrain)\n",
    "print(f'The chosen features are: {feature_names[S]}')\n",
    "n_features = len(S)\n",
    "plt.figure()\n",
    "plt.plot(np.arange(n_features) + 1, best_acc_list)\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(np.arange(n_features) + 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1083071a",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> TODO: </span> Discuss:\n",
    "1. Which features were added? Note the order in which they were added. Does that surprise you?\n",
    "2. Is there any plateauing behavior? If so, can you make a conclusion about the set of features that **really** mattered?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ef63d",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> TODO: </span> Evaluate the performance using the chosen set of features on the test set. Be careful about which set you fit your model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c760d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c0fcd8",
   "metadata": {},
   "source": [
    "## Task 2C.2. L-1 SVM (10 pts)\n",
    "\n",
    "In this task, you will implement the L1-SVM. For sake of simplicity, kernelized SVM will not be necessary here.\n",
    "\n",
    "The loss function for the L1-SVM is\n",
    "\n",
    "$$\n",
    "    L = \\frac{1}{n} \\sum_{i=1}^n \\max(0, 1 - y_i \\mathbf{w}^T \\mathbf{x}_i) + \\lambda \\lVert w \\rVert_1 \n",
    "$$\n",
    "\n",
    "where we have the hinge loss and a $L_1$ regularization term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88997e64",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> TODO: </span> Implement the L1-SVM loss and grad functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6c54ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFunctionL1SVM(w, X, y, lambda_):\n",
    "    #### TODO: YOUR CODE HERE\n",
    "    return \n",
    "\n",
    "def gradientDescent(X, y, initial_w, eta, tolerance, lambda_, max_iter=1000):\n",
    "    Loss_history =[]\n",
    "    prev_loss = np.inf\n",
    "    w = initial_w\n",
    "    for _ in range(int(max_iter)):\n",
    "        loss, grad = lossFunctionL1SVM(w, X, y, lambda_)\n",
    "        Loss_history.append(loss)\n",
    "        w = w - eta * grad\n",
    "        if abs(prev_loss - loss) < tolerance:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "    \n",
    "    return w, Loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a99e4",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> TODO: </span> Find the best lambda using cross validation. Plot the cross-validated L1-SVM loss and test L1-SVM loss against lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "28150c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(Xtrain, ytrain, initial_w, eta, tolerance, lambda_, max_iter=1000, k = 10):\n",
    "    m = len(ytrain)\n",
    "    fold_size = m//k\n",
    "    loss_list = []\n",
    "    for i in range(k):\n",
    "        tr_idx = list(range(i*fold_size)) + list(range((i+1) * fold_size, m))\n",
    "        val_idx = list(range(i * fold_size, (i+1) * fold_size))\n",
    "        #### TODO: YOUR CODE HERE\n",
    "        \n",
    "    return np.mean(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72907c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_list = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "\n",
    "cv_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "initial_w = np.zeros(Xtrain.shape[1])\n",
    "eta = 0.01\n",
    "tolerance = 1e-4\n",
    "\n",
    "w_list = []\n",
    "for lambda_ in lambda_list:\n",
    "    #### TODO: YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lambda_list, cv_loss_list, label='Cross Validation Loss', marker='o')\n",
    "plt.plot(lambda_list, test_loss_list, label='Test Loss', marker='x')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Cross Validation and Test Loss vs Lambda')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cfa246",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> TODO: </span> Evaluate the accuracy of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa17abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO: YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

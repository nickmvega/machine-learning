{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Representation Learning\n",
    "\n",
    "## Objectives\n",
    "Your goal in this project is to get comfortable in implementing k-means, PCA, and neural network. To complete this project, you should understand the following:\n",
    "\n",
    "* How to use basic math and machine learning modules in python such as numpy\n",
    "* How to fit a k-means algorithm *from scratch*\n",
    "* How to perform PCA (principal component analysis) *from scratch* on MNIST dataset\n",
    "* How to train a neural network *from scratch* on MNIST dataset\n",
    "* How to evaluate the test results and visualize the outcome of these ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable\n",
    "* Project report/writeup: A `project3_report_lastname.pdf` file with corresponding plots and results for the project. Follow the `Project 3 - Report (Individual Submission)` link on Gradescope to upload this file. Also, please put your name on the report so that the grader can find your code easier on the Gradescope. \n",
    "\n",
    "    The project report should include a brief justification of your solution at a high-level, e.g., using any relevant explanations, equations, or pictures that help to explain your solution. You should **answer each question sequenctially**, and use clearly labelled, separate paragraphs for each question. You should also **describe what your code does**, e.g. using a couple of sentences per function to describe your code structure. The objective is to make the report self-contained for grading.\n",
    "\n",
    "    * To be more specific, in addition to textual descriptions and explanations for each sections, you should include images/screenshots/code blocks of:\n",
    "        1. Any code snippets you implemented\n",
    "        2. Plots affected by your implementation, e.g., scatter plots with decision boundaries or loss curves.\n",
    "        3. Output results of the questions, e.g., the best lambda from cross validation.\n",
    "        4. Please do NOT include debugging messages.\n",
    "\n",
    "\n",
    "* Source code: A `project3_src_lastname1[_lastname2].ipynb` (or `.zip`) file with a working copy of your solutions compiled in a Jupyter notebook. Follow the `Project 3 - Source Code (Group Submission)` link to upload this file.\n",
    "    * You are asked to complete code snippets in the following two formats:\n",
    "        1. Between comments of `Your code starts here` and `End of Your Code`.\n",
    "        2. Inline comments with `Your code here`.\n",
    "\n",
    "\n",
    "## Logistics\n",
    "\n",
    "* You can work in groups of 1-2 students for each course project, and it's your responsibility to find a group (e.g. use Ed Discussion). \n",
    "* Every member of a group must complete and submit the project report/writeup individually. While the source code can be the same for all group members, the project report needs to be written independently by each person and, thus, should differ among team member and students more generally.\n",
    "* One group member need to submit the source code. If you submit as a group, make sure to include your teammate in the group submission. Instructions for team submission can be found [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "* Grades will be provided based on the individual project report. The source code submission will not be graded, but the teaching staff may check the source files if they see the need for reproducing your results when going through your project report. \n",
    "* Failure to submit the source code will lead to a deduction of points from your total.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3A: K-Means (30 points)\n",
    "K-Means is an unsupervised ML algorithm that segments data into groups based on their similarities. In this part, we will implement K-Means with different seeding algorithms from scratch (using only numpy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "We will use 2-dimensional data in this section. Specifically, the dataset are\n",
    "2D points $[(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)]$ that form 5 distinct clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /Users/nicholasvega/Downloads/machine-learning/venv/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/nicholasvega/Downloads/machine-learning/venv/lib/python3.13/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/nicholasvega/Downloads/machine-learning/venv/lib/python3.13/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/nicholasvega/Downloads/machine-learning/venv/lib/python3.13/site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/nicholasvega/Downloads/machine-learning/venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/nicholasvega/Downloads/machine-learning/venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/nicholasvega/Downloads/machine-learning/venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/nicholasvega/Downloads/machine-learning/venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nicholasvega/Downloads/machine-learning/venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/nicholasvega/Downloads/machine-learning/venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/nicholasvega/Downloads/machine-learning/venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/nicholasvega/Downloads/machine-learning/venv/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nicholasvega/Downloads/machine-learning/venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/nicholasvega/Downloads/machine-learning/venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nicholasvega/Downloads/machine-learning/venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install seaborn for better visualization\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare and plot the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_blobs\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_samples = 1000\n",
    "num_centers = 5\n",
    "\n",
    "\n",
    "X_train, true_labels = make_blobs(\n",
    "    n_samples=num_samples,\n",
    "    centers=num_centers,\n",
    "    random_state=80,\n",
    ")\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "sns.scatterplot(\n",
    "    x=[X[0] for X in X_train],\n",
    "    y=[X[1] for X in X_train],\n",
    "    hue=true_labels,\n",
    "    palette=\"deep\",\n",
    "    legend=None,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Implementation\n",
    "Now we are implementing the algorithm.\n",
    "\n",
    "First, we define \"similarity\" between points. For our dataset, we will be using Euclidean distances.\n",
    "\n",
    "1. **Implement a helper function** that has two inputs: dataset, cur_point, and returns the Euclidean distance between cur_point and each point in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_dist(cur_point, dataset):\n",
    "    '''\n",
    "    cur_point has dimensions (m,), dataset has dimensions (n, m), and output will be of size (n,).\n",
    "    '''\n",
    "    dists = None\n",
    "    # Your Code Starts Here\n",
    "    dists = np.linalg.norm(dataset - cur_point, axis=1)\n",
    "    # End of Your Code\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement the actual K-Means algorithm. We first initialize K-Means with two parameters: number of clusters, and max number of iteractions, which you will use in the init function. \n",
    "\n",
    "1. **Implent the initialization functions** to accommodate four different seeding strategies: first k, random, max distance, and k-means++.\n",
    "2. **Implement the fit function** which takes the dataset you created earlier as input. \n",
    "\n",
    "Finally, we can assign data points to the closest centroid in the provided predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import random\n",
    "from numpy.random import uniform\n",
    "\n",
    "class InitializationMethod(Enum):\n",
    "    FirstK = 1\n",
    "    Random = 2\n",
    "    MaxDistance = 3\n",
    "    KMeansPlusPlus = 4\n",
    "\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, n_clusters : int = 5, max_iter : int = 1000):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.centroids = None\n",
    "\n",
    "    def initialize_and_fit(self,\n",
    "                           initialization_method : InitializationMethod,\n",
    "                           X_train : np.ndarray):\n",
    "        # Initialize based on provided method choice.\n",
    "        if initialization_method == InitializationMethod.FirstK:\n",
    "            self.centroids = self.__initialize_first_k(X_train)\n",
    "        elif initialization_method == InitializationMethod.Random:\n",
    "            self.centroids = self.__initialize_random(X_train)\n",
    "        elif initialization_method == InitializationMethod.MaxDistance:\n",
    "            self.centroids = self.__initialize_max_distance(X_train)\n",
    "        elif initialization_method == InitializationMethod.KMeansPlusPlus:\n",
    "            self.centroids = self.__initialize_kmeanspp(X_train)\n",
    "\n",
    "        # Some validation to be sure your methods are at least somewhat behaving.\n",
    "        assert not self.centroids is None  # The centroids were set.\n",
    "        assert len(self.centroids) == self.n_clusters  # n_clusters of them were chosen.\n",
    "\n",
    "        # Fit to the given centroids.\n",
    "        num_iterations_needed = self.__fit(X_train)\n",
    "        return num_iterations_needed\n",
    "\n",
    "    def __initialize_first_k(self, X_train : np.ndarray):\n",
    "        \"\"\"\n",
    "        Initialize the centroids by selecting the first k points in X_train.\n",
    "        \"\"\"\n",
    "        centroids = []\n",
    "\n",
    "        # YOUR CODE STARTS HERE\n",
    "        for i in range(self.n_clusters):\n",
    "            centroids.append(X_train[i])\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        return centroids\n",
    "\n",
    "    def __initialize_random(self, X_train : np.ndarray):\n",
    "        \"\"\"\n",
    "        Initialize the centroids by selecting k different points uniformly at\n",
    "        random from X_train.\n",
    "        \"\"\"\n",
    "        centroids = []\n",
    "\n",
    "        # YOUR CODE STARTS HERE\n",
    "        uni = uniform(0, len(X_train), self.n_clusters)\n",
    "\n",
    "        for i in range(uni):\n",
    "            centroids.append(X_train[i])\n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        return centroids\n",
    "\n",
    "    def __initialize_max_distance(self, X_train : np.ndarray):\n",
    "        \"\"\"\n",
    "        Initialize the centroids by iteratively selecting the furthest point.\n",
    "        First, a random datapoint is selected as the first centroid, and then\n",
    "        at each step (until k have been selected) the point in X_train that is\n",
    "        of maximum distance from any existing cluster center is selected.\n",
    "        \"\"\"\n",
    "        # Pick a random point from train data for first centroid.\n",
    "        centroids = [random.choice(X_train)]\n",
    "\n",
    "        # Then choose the remaining points by selecting the point with maximum\n",
    "        # euclidian distance to any current centroid.\n",
    "        #\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        return centroids\n",
    "\n",
    "    def __initialize_kmeanspp(self, X_train : np.ndarray):\n",
    "        \"\"\"\n",
    "        Initialize the centroids using the \"k-means++\" method, where a random\n",
    "        datapoint is selected as the first, then the rest are initialized w/\n",
    "        probabilities proportional to their distances to the first.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pick a random point from train data for first centroid.\n",
    "        centroids = [random.choice(X_train)]\n",
    "\n",
    "        # Then choose the remaining points as per kMeans++ algorithm.\n",
    "        #\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "        return centroids\n",
    "\n",
    "    def __fit(self, X_train : np.ndarray):\n",
    "        \"\"\"\n",
    "        Iterate, adjusting centroids until converged (new centroids are the same\n",
    "        as previous centroids) or until passed max_iter.\n",
    "\n",
    "        Returns the number of iterations needed for the choice of centroids to\n",
    "        stabilize.\n",
    "        \"\"\"\n",
    "        iteration_count = 0\n",
    "        prev_centroids = None\n",
    "\n",
    "        while np.not_equal(self.centroids, prev_centroids).any() and iteration_count < self.max_iter:\n",
    "            # Sort each datapoint, assigning to nearest centroid\n",
    "            # Push current centroids to previous, reassign centroids as mean of the points belonging to them\n",
    "            #\n",
    "            # YOUR CODE STARTS HERE\n",
    "            \n",
    "            # YOUR CODE ENDS HERE\n",
    "            iteration_count += 1\n",
    "\n",
    "        return iteration_count\n",
    "\n",
    "    def predict(self, X : np.ndarray):\n",
    "        \"\"\"\n",
    "        Assign each data point to the nearest centroid.\n",
    "        \"\"\"\n",
    "        centroids = []\n",
    "        centroid_idxs = []\n",
    "        for x in X:\n",
    "            dists = euclidean_dist(x, self.centroids)\n",
    "            centroid_idx = np.argmin(dists)\n",
    "            centroids.append(self.centroids[centroid_idx])\n",
    "            centroid_idxs.append(centroid_idx)\n",
    "\n",
    "        return centroids, centroid_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **test and visualize your k-means algorithm** with various initializations. In the below given code, we separate the different true labels by color (as previously), and we distinguish predicted labels by marker styles.\n",
    "\n",
    "*You should make sure all centroids (\"+\" signs) are in different clusters. Since initialization is important for k-means, so this may not be true for every run.*\n",
    "\n",
    "The below code will generate 3 charts, each containing 4 sub-plots. **Include at least one of them in your report, and comment on your observations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_centers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Run k-means with each initialization method.\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m model \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[43mnum_centers\u001b[49m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     52\u001b[0m     plot_predictions(model, [X[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m X_train], [X[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m X_train])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_centers' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "initializetion_methods = [\n",
    "    (InitializationMethod.FirstK, \"First K\"),\n",
    "    (InitializationMethod.Random, \"Random\"),\n",
    "    (InitializationMethod.MaxDistance, \"Max Distance\"),\n",
    "    (InitializationMethod.KMeansPlusPlus, \"k-Means++\")\n",
    "]\n",
    "\n",
    "def plot_predictions(model : KMeans, X : np.ndarray, y : np.ndarray):\n",
    "    \"\"\"Generates a single chart with subplots for multiple model prediction runs.\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 8))  # 2x2 grid of subplots\n",
    "    axes = axes.flatten()  # Flatten the axes array for easier indexing\n",
    "\n",
    "    for i in range(len(initializetion_methods)):\n",
    "        method, name = initializetion_methods[i]\n",
    "\n",
    "        # Initialize with given method.\n",
    "        model.initialize_and_fit(method, X_train)\n",
    "\n",
    "        # View results\n",
    "        class_centers, classification = model.predict(X_train)\n",
    "\n",
    "        # plot the ground truths\n",
    "        sns.scatterplot(\n",
    "            x=X,\n",
    "            y=y,\n",
    "            hue=true_labels,\n",
    "            style=classification,\n",
    "            palette=\"deep\",\n",
    "            legend=None,\n",
    "            ax=axes[i]\n",
    "        )\n",
    "        # plot the centroids\n",
    "        axes[i].plot(\n",
    "            [x for x, _ in class_centers],\n",
    "            [y for _, y in class_centers],\n",
    "            '+',\n",
    "            markersize=20,\n",
    "        )\n",
    "\n",
    "        axes[i].set_title(name)\n",
    "\n",
    "    plt.tight_layout()  # Adjust subplot params for a tight layout\n",
    "    plt.show()\n",
    "\n",
    "# Run k-means with each initialization method.\n",
    "model = KMeans(n_clusters=num_centers)\n",
    "for _ in range(3):\n",
    "    plot_predictions(model, [X[0] for X in X_train], [X[1] for X in X_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of these different methods can be seen above, but that's only the _end result_! As you can see in your code, each of these initialization methods is of varying complexity. \n",
    "\n",
    "From each initialization state, how many steps does it take to complete initialization? **Include this graph in your report, and comment on the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "iterations = 20\n",
    "\n",
    "# Run k-means with each initialization method.\n",
    "model = KMeans(n_clusters=num_centers)\n",
    "\n",
    "data = []\n",
    "for i in range(len(initializetion_methods)):\n",
    "    method, name = initializetion_methods[i]\n",
    "\n",
    "    num_steps = []\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # Initialize with given method.\n",
    "        num_steps.append(model.initialize_and_fit(method, X_train))\n",
    "\n",
    "    data.append((num_steps, name))\n",
    "\n",
    "sns.violinplot([d[0] for d in data])\n",
    "plt.xticks(np.arange(len(data)), labels=[d[1] for d in data])\n",
    "plt.title(\"Number of steps to converge centroids\")\n",
    "plt.ylabel(\"Number of steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "In practice, both k-Means++ and Max Distance are used. In what conditions might you prefer one of these methods over another? What are the drawbacks of each method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3B: Principal Component Analysis (30 points)\n",
    "\n",
    "In this problem you will apply PCA for dimensionality reduction to the MNIST dataset.\n",
    "\n",
    "Note that **you must implement PCA directly with Numpy**, you are not allowed to use the PCA class from scikit-learn or any other PCA/dimensionality reduction library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset and visualize some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Loading and preprocessing data')\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False, data_home='data')\n",
    "y = y.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some random samples from training set\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(3, 3, figsize=(5, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f'class: {y_train[i]}')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple linear classifier\n",
    "\n",
    "This class has been implemented for you. You don't need to change code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't Change this cell\n",
    "class LinearClassifier:\n",
    "    \"\"\"\n",
    "    Simple linear classifier trained using full-batch gradient descent.\n",
    "    - Maps n_features -> n_classes logits using a weight matrix W and bias vector b.\n",
    "    - Applies softmax to the logits to get class probabilities that sum to 1.\n",
    "    - In prediction, we take the class with the highest probability as our output class.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, classes=np.arange(10), eps=1e-4):\n",
    "        self.classes = classes\n",
    "        self.W = np.random.rand(n_features, len(classes)) / 30\n",
    "        self.b = np.random.rand(len(classes)) / 30\n",
    "        self.eps = eps\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        assert len(X) == len(y)\n",
    "        assert set(y) == set(self.classes)\n",
    "\n",
    "        for i in range(200):\n",
    "            # forward pass\n",
    "            z = X @ self.W + self.b\n",
    "\n",
    "            # softmax to convert logits to class probabilities\n",
    "            exp_z = np.exp(z)\n",
    "            softmax = exp_z / exp_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "            # cross-entropy loss\n",
    "            n = len(X)\n",
    "            loss = -np.log(softmax[range(n), y]).sum() / n\n",
    "\n",
    "            # backward pass\n",
    "            dL_dz = softmax\n",
    "            dL_dz[range(n), y] -= 1\n",
    "            dL_dz /= n\n",
    "\n",
    "            dL_dW = X.T @ dL_dz\n",
    "            dL_db = dL_dz.sum(axis=0)\n",
    "\n",
    "            # update weights\n",
    "            self.W -= self.eps * dL_dW\n",
    "            self.b -= self.eps * dL_db\n",
    "\n",
    "            # print classification accuracy\n",
    "            if i % 50 == 0:\n",
    "                predictions = np.argmax(z, axis=1)\n",
    "                accuracy = (predictions == y).mean()\n",
    "                print(f'iteration {i}: loss {loss:.4f}, accuracy {accuracy:.4f}')\n",
    "        predictions = np.argmax(z, axis=1)\n",
    "        accuracy = (predictions == y).mean()\n",
    "        print(f'Final iteration {i}: loss {loss:.4f}, accuracy {accuracy:.4f}')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        z = X @ self.W + self.b\n",
    "        return np.argmax(z, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As a baseline, let's report the accuracy of a simple linear classifier trained with full-batch gradient descent**. Instead of using all 28 x 28 = 784 pixels as input, we'll uniformly sample 27 pixels from the image and see what our accuracy is.\n",
    "\n",
    "You should see something around ~57% train accuracy and ~57% test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample every 30 pixels\n",
    "X_train_subset = X_train[:, ::30]\n",
    "print(f'Using {X_train_subset.shape[1]} features')\n",
    "\n",
    "classifier = LinearClassifier(n_features=X_train_subset.shape[1])\n",
    "\n",
    "# center the data and store the means\n",
    "feat_means = X_train_subset.mean(axis=0, keepdims=True)\n",
    "X_train_subset = X_train_subset - feat_means\n",
    "classifier.fit(X_train_subset, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our test accuracy\n",
    "X_test_subset = X_test[:, ::30]\n",
    "preds = classifier.predict(X_test_subset - feat_means)\n",
    "test_acc = (preds == y_test).mean()\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for dimensionality reduction in classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use PCA to choose a more effective set of features? \n",
    "\n",
    "Following the starter code below to implement PCA:\n",
    "\n",
    "- First, **implement the fit function** which takes the data as input, and compute and store the truncated SVD into self.U_subset, self.S_subset, self.Vt_subset.\n",
    "\n",
    "- Then, **implement the predict function**, which projects the data onto our kept principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, n_components):\n",
    "        # number of principal components to keep when we apply PCA\n",
    "        assert n_components <= 784\n",
    "        self.n_components = n_components\n",
    "\n",
    "        self.feature_means = None\n",
    "        self.U, self.S, self.Vt = None, None, None\n",
    "        self.U_subset, self.S_subset, self.Vt_subset = None, None, None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Steps:\n",
    "        - Center the data (compute, store, and subtract the mean of each feature)\n",
    "        - Take the SVD of the centered data\n",
    "        - Truncate the SVD by keeping only the first n_components (singular values)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        ## Your Code Starts Here ##\n",
    "        # center the data per feature\n",
    "\n",
    "        # take SVD\n",
    "\n",
    "        # store truncated SVD\n",
    "                \n",
    "        ## End of Your Code ##\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Steps:\n",
    "        - Center the data (subtract the mean of each feature, stored in the fit)\n",
    "        - Project the centered data onto the principal components\n",
    "\n",
    "        Returns:\n",
    "        - Projected data of shape (X.shape[0], n_components)\n",
    "        \"\"\"\n",
    "        if self.feature_means is None:\n",
    "            raise ValueError('fit the PCA model first')\n",
    "\n",
    "        ## Your Code Starts Here ##\n",
    "        # center X\n",
    "\n",
    "        # project the centered data set onto our kept principal components\n",
    "\n",
    "        return \n",
    "        ## End of Your Code ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit PCA\n",
    "pca = PCA(27)\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the explained variance ratio\n",
    "S = pca.S\n",
    "plt.plot(S.cumsum() / S.sum())\n",
    "\n",
    "# report the truncated feature ratio\n",
    "explained_var_ratio = (S.cumsum() / S.sum())[pca.n_components]\n",
    "print(f'Explained variance ratio with {pca.n_components} components: {explained_var_ratio:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_subset = pca.predict(X_train)\n",
    "X_test_subset = pca.predict(X_test)\n",
    "\n",
    "classifier = LinearClassifier(n_features=27)\n",
    "classifier.fit(X_train_subset, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate our test accuracy with PCA.** You are expected to achieve a higher accuracy ~80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = classifier.predict(X_test_subset)\n",
    "test_acc = (preds == y_test).mean()\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for visualization\n",
    "The following block of code will visualize the first 2 principal components. You should discuss what you see by comparing the clusters of different digits in the 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the first two PCs from the training set and plot it\n",
    "X_train_subset = pca.predict(X_train)\n",
    "X_train_subset = X_train_subset[:, :2]\n",
    "\n",
    "plt.scatter(X_train_subset[:, 0], X_train_subset[:, 1], c=y_train, cmap='tab10', alpha=1.0, s=1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3C: Neural Network (40 points + 20 bonus points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal in this exercise is to classify\n",
    "handwritten digits with neural networks. The training and test units, together with the\n",
    "structure of a multilayer perceptron class are already provided in the starter code. Your task\n",
    "is to code up the key lines that complete the **forward** and **backward** propagation functions.\n",
    "\n",
    "\n",
    "**forward**: Recall that forward propagation corresponds to the prediction algorithm for neural nets. The\n",
    "intermediate outputs for this method will be cached for backpropagation during the training phase.\n",
    "\n",
    "**backward**: Backpropagation collects all the gradients necessary for training the neural network, which will be\n",
    "used as input for the minibatch SGD algorithm GradientDescentOptimizer\n",
    "\n",
    "In the next, you will need to implement the forward pass and backward pass of the linear layer. For the forward pass, it takes $x_l$ as input and outputs $x_{l+1}$.\n",
    "\n",
    "$$x_{l+1} = w^\\top x_{l} + b_{l}$$\n",
    "\n",
    "For the backward pass, you will need to compute the backward gradients,\n",
    "\n",
    "$$ \\frac{\\partial \\ell(x, y)}{\\partial w}=?$$ \n",
    "$$\\frac{\\partial \\ell(x, y)}{\\partial b}=?$$\n",
    "$$\\frac{\\partial \\ell(x, y)}{\\partial x_{l}}=?$$\n",
    "\n",
    "where $\\ell$ is the loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3C.1: Forward and Backward Propogation (20pt)\n",
    "\n",
    "**Implement the forward and backward pass of the linear layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, shape, activ_func):\n",
    "        \"Implements a layer of a NN.\"\n",
    "      \n",
    "        self.w = np.random.uniform(-np.sqrt(2.0 / shape[0]),\n",
    "                                   np.sqrt(2.0 / shape[0]),\n",
    "                                   size=shape)\n",
    "        self.b = np.zeros((1, shape[1]))\n",
    "\n",
    "        # The activation function, for example, RELU, tanh, or sigmoid.\n",
    "        self.activate = activ_func\n",
    "\n",
    "        # The derivative of the activation function.\n",
    "        self.d_activate = GRAD_DICT[activ_func]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward propagate the activation through the layer.\n",
    "        \n",
    "        Given the inputs (activation of previous layers),\n",
    "        compute and save the activation of current layer,\n",
    "        then return it as output.\n",
    "        \"\"\"\n",
    "\n",
    "        ###################################Description begins#################\n",
    "        # Forward pass\n",
    "\n",
    "        # Use the linear and non-linear transformation to\n",
    "        # compute the activation and cache it in a the field, self.a.\n",
    "\n",
    "        # Functions you may use:\n",
    "        # np.dot: numpy function to compute dot product of two matrix.\n",
    "        # self.activate: the activation function of this layer,\n",
    "        #                it takes in a matrix of scores (linear transformation)\n",
    "        #                and compute the activations (non-linear transformation).\n",
    "        # (plus the common arithmetic functions).\n",
    "\n",
    "        # For all the numpy functions, use google and numpy manual for\n",
    "        # more details and examples.        \n",
    "        \n",
    "        # Object fields you will use:\n",
    "        # self.w:\n",
    "        #     weight matrix, a matrix with shape (H_-1, H).\n",
    "        #     H_-1 is the number of hidden units in previous layer\n",
    "        #     H is the number of hidden units in this layer\n",
    "        # self.b: bias, a matrix/vector with shape (1, H).\n",
    "        # self.activate: the activation function of this layer.\n",
    "\n",
    "        # Input:\n",
    "        # inputs:\n",
    "        #    a matrix with shape (N, H_-1),\n",
    "        #    N is the number of data points.\n",
    "        #    H_-1 is the number of hidden units in previous layer\n",
    "\n",
    "        ###################################Description ends####################\n",
    "        # Modify the right hand side of the following code.\n",
    "        \n",
    "        # The linear transformation.\n",
    "        # scores:\n",
    "        #     weighted sum of inputs plus bias, a matrix of shape (N, H).\n",
    "        #     N is the number of data points.\n",
    "        #     H is the number of hidden units in this layer.\n",
    "\n",
    "        scores = None ## Your code here ##\n",
    "        # End of your code\n",
    "         \n",
    "\n",
    "        # The non-linear transformation.\n",
    "        # outputs:\n",
    "        #     activations of this layer, a matrix of shape (N, H).\n",
    "        #     N is the number of data points.\n",
    "        #     H is the number of hidden units in this layer.\n",
    "\n",
    "        activations = None ## Your code here ##\n",
    "        # End of your code\n",
    "\n",
    "        # End of the code to modify\n",
    "        #########################################################\n",
    "\n",
    "        # Cache the inputs and the activations (to be used by backprop).\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        self.a = activations\n",
    "        outputs = activations\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, d_outputs):\n",
    "        \"\"\"Backward propagate the gradient through this layer.\n",
    "        \n",
    "        Given the gradient w.r.t the output of this layer\n",
    "        (d_outputs), compute and save the gradient w.r.t the\n",
    "        weights (d_w) and bias (d_b) of this layer and\n",
    "        return the gradient w.r.t the inputs (d_inputs).\n",
    "        \"\"\"\n",
    "        ###################################Description begins#################\n",
    "        # Backpropagation\n",
    "\n",
    "        # Compute the derivatives of the loss w.r.t the weights and bias\n",
    "        # given the derivatives of the loss w.r.t the outputs of this layer\n",
    "        # using chain rule.\n",
    "\n",
    "        # Naming convention: use d_var to store the\n",
    "        # derivative of the loss w.r.t the variable.\n",
    "        \n",
    "        # Functions you may use:\n",
    "        # np.dot (numpy.dot): numpy function to compute dot product of two matrix.\n",
    "        # np.mean or np.sum (numpy.mean or numpy.sum):\n",
    "        #     numpy function to compute the mean or sum of a matrix,\n",
    "        #     use keywords argument 'axis' to compute the mean\n",
    "        #     or sum along a particular axis, you might also\n",
    "        #     found 'keepdims' argument useful.\n",
    "        # self.d_activate:\n",
    "        #     given the current activation (self.a) as input,\n",
    "        #     compute the derivative of the activation function,\n",
    "        #     See d_relu as an example.\n",
    "        # (plus the common arithmetic functions).\n",
    "        # np.transpose or m.T (m is an numpy array): transpose a matrix.\n",
    "        \n",
    "        \n",
    "        # Object fields you will use:\n",
    "        # self.w: weight matrix, a matrix with shape (H_-1, H).\n",
    "        #         H_-1 is the number of hidden units in previous layer\n",
    "        #         H is the number of hidden units in this layer\n",
    "        # self.d_activate: compute derivative of the activation function.\n",
    "        #                  See d_relu as an example.\n",
    "        # d_outputs: the derivative of the loss w.r.t the outputs of\n",
    "        #            this layer, a matrix of shape (N, H). N is the number of\n",
    "        #            data points and H is the number of hidden units in this layer.\n",
    "        # self.inputs: inputs to this layer, a matrix with shape (N, H_-1)\n",
    "        #              N is the number of data points.\n",
    "        #              H_-1 is the number of hidden units in previous layer.\n",
    "        # self.a: activation of the hidden units of this layer, a matrix\n",
    "        #         with shape (N, H)\n",
    "        #         N is the number of data points.\n",
    "        #         H is the number of hidden units in this layer.\n",
    "\n",
    "        ###################################Description ends####################\n",
    "        \n",
    "        \n",
    "        # Modify the right hand side of the following code.\n",
    "\n",
    "        # d_scores:\n",
    "        #     Derivatives of the loss w.r.t the scores (the result from linear transformation).\n",
    "        #     A matrix of shape (N, H)\n",
    "        #     N is the number of data points.\n",
    "        #     H is the number of hidden units in this layer.\n",
    "        d_scores = None ## Your code here ##\n",
    "        # End of your code\n",
    "\n",
    "        # self.d_b:\n",
    "        #     Derivatives of the loss w.r.t the bias, averaged over all data points.\n",
    "        #     A matrix of shape (1, H)\n",
    "        #     H is the number of hidden units in this layer.\n",
    "\n",
    "        self.d_b = None ## Your code here ##\n",
    "        # End of your code\n",
    "\n",
    "        # self.d_w:\n",
    "        #     Derivatives of the loss w.r.t the weight matrix, averaged over all data points.\n",
    "        #     A matrix of shape (H_-1, H)\n",
    "        #     H_-1 is the number of hidden units in previous layer\n",
    "        #     H is the number of hidden units in this layer.      \n",
    "          \n",
    "        self.d_w = None ## Your code here ##\n",
    "        # End of your code\n",
    "\n",
    "        # d_inputs:\n",
    "        #     Derivatives of the loss w.r.t the previous layer's activations/outputs.\n",
    "        #     A matrix of shape (N, H_-1)\n",
    "        #     N is the number of data points.\n",
    "        #     H_-1 is the number of hidden units in the previous layer.\n",
    "        \n",
    "        d_inputs = None ## Your code here ##\n",
    "        # End of your code\n",
    "\n",
    "        # End of the code to modify\n",
    "        #########################################################\n",
    "        \n",
    "\t\t# Compute the average value of the gradients, since\n",
    "        # we are minimizing the average loss. \n",
    "        self.d_b /= d_scores.shape[0]\n",
    "        self.d_w /= d_scores.shape[0]\n",
    "        \n",
    "        return d_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have successfully completed task 3C-I. Please run the code to define the Multi-Layer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "    \n",
    "#########################\n",
    "#  Multilayer Perceptron\n",
    "#########################\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, input_dim, output_dim, sizes, activ_funcs):\n",
    "        \"\"\"Multilayer perceptron for multi-class classification.\n",
    "\n",
    "        The object holds a list of layer objects, each one\n",
    "        implements a layer in the network, the specification\n",
    "        of each layer is decided by input_dim, output_dim,\n",
    "        sizes and activ_funcs. Note that an output layer\n",
    "        (linear) and loss function (softmax and\n",
    "        cross-entropy) would be automatically added to the MLP.\n",
    "\n",
    "        Input: \n",
    "          input_dim: dimension of input.\n",
    "          output_dim: dimension of output (number of labels).\n",
    "          sizes: a list of integers specifying the number of\n",
    "            hidden units on each layer.\n",
    "          activ_funcs: a list of function objects specifying\n",
    "            the activation function of each layer.\n",
    "\n",
    "        \"\"\"\n",
    "        # Last layer is linear and loss is mean_cross_entropy_softmax\n",
    "        self.sizes = [input_dim] + sizes[:] + [output_dim]\n",
    "        self.activ_funcs = activ_funcs[:] + [linear]\n",
    "        self.shapes = []\n",
    "        for i in range(len(self.sizes)-1):\n",
    "            self.shapes.append((self.sizes[i], self.sizes[i+1]))\n",
    "\n",
    "        self.layers = []\n",
    "        for i, shape in enumerate(self.shapes):\n",
    "            self.layers.append(Layer(shape, self.activ_funcs[i]))\n",
    "\n",
    "    def forwardprop(self, data, labels=None):\n",
    "        \"\"\"Forward propagate the activations through the network.\n",
    "\n",
    "        Iteratively propagate the activations (starting from\n",
    "        input data) through each layer, and output a\n",
    "        probability distribution among labels (probs), and\n",
    "        if labels are given, also compute the loss. \n",
    "        \"\"\"\n",
    "        inputs = data\n",
    "        for layer in self.layers:\n",
    "            outputs = layer.forward(inputs)\n",
    "            inputs = outputs\n",
    "            \n",
    "        probs = softmax(outputs)\n",
    "        if labels is not None:\n",
    "            return probs, self.loss(outputs, labels)\n",
    "        else:\n",
    "            return probs, None\n",
    "\n",
    "    def backprop(self, labels):\n",
    "        \"\"\"Backward propagate the gradients/derivatives through the network.\n",
    "        \n",
    "        Iteratively propagate the gradients/derivatives (starting from\n",
    "        outputs) through each layer, and save gradients/derivatives of\n",
    "        each parameter (weights or bias) in the layer.\n",
    "        \"\"\"\n",
    "        d_outputs = self.d_loss(self.layers[-1].a, labels)\n",
    "        for layer in self.layers[::-1]:\n",
    "            d_inputs = layer.backward(d_outputs)\n",
    "            d_outputs = d_inputs\n",
    "\n",
    "    def loss(self, outputs, labels):\n",
    "        \"Compute the cross entropy softmax loss.\"\n",
    "        return mean_cross_entropy_softmax(outputs, labels)\n",
    "\n",
    "    def d_loss(self, outputs, labels):\n",
    "        \"Compute derivatives of the cross entropy softmax loss w.r.t the outputs.\"\n",
    "        return d_mean_cross_entropy_softmax(outputs, labels)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        \"Predict the labels of the data.\"\n",
    "        probs, _ = self.forwardprop(data)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "\n",
    "class GradientDescentOptimizer(object):\n",
    "    def __init__(self, learning_rate, decay_steps=1000,\n",
    "                 decay_rate=1.0):\n",
    "        \"Gradient descent with staircase exponential decay.\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.steps = 0.0\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "    def update(self, model):\n",
    "        \"Update model parameters.\"\n",
    "        for layer in model.layers:\n",
    "            layer.w -= layer.d_w * self.learning_rate\n",
    "            layer.b -= layer.d_b * self.learning_rate\n",
    "        self.steps += 1\n",
    "        if (self.steps + 1) % self.decay_steps == 0:\n",
    "            self.learning_rate *= self.decay_rate\n",
    "\n",
    "\n",
    "# Utility functions.\n",
    "def sigmoid(x): \n",
    "    return 1/(1+np.exp(-x))   \n",
    "\n",
    "def d_sigmoid(a=None, x=None):\n",
    "    if a is not None:\n",
    "        return a * (1 - a)\n",
    "    else:\n",
    "        return d_sigmoid(a=sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    \"The rectified linear activation function.\"\n",
    "    return np.clip(x, 0.0, None)\n",
    "\n",
    "def d_relu(a=None, x=None):\n",
    "    \"Compute the derivative of RELU given activation (a) or input (x).\"\n",
    "    if a is not None:    \n",
    "        d = np.zeros_like(a)\n",
    "        d[np.where(a > 0.0)] = 1.0\n",
    "        return d\n",
    "    else:\n",
    "        return d_relu(a=relu(x))\n",
    "\n",
    "def tanh(x):\n",
    "    \"The tanh activation function.\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_tanh(a=None, x=None):\n",
    "    \"The derivative of the tanh function.\"\n",
    "    if a is not None:\n",
    "        return 1 - a ** 2\n",
    "    else:\n",
    "        return d_tanh(a=tanh(x))\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def d_linear(a=None, x=None):\n",
    "    return 1.0\n",
    "\n",
    "def softmax(x):\n",
    "    shifted_x = x - np.max(x, axis=1, keepdims=True)\n",
    "    f = np.exp(shifted_x)\n",
    "    p = f / np.sum(f, axis=1, keepdims=True)\n",
    "    return p\n",
    "    \n",
    "def mean_cross_entropy(outputs, labels):\n",
    "    n = labels.shape[0]\n",
    "    return - np.sum(labels * np.log(outputs)) / n\n",
    "\n",
    "def mean_cross_entropy_softmax(logits, labels):\n",
    "    return mean_cross_entropy(softmax(logits), labels)\n",
    "\n",
    "def d_mean_cross_entropy_softmax(logits, labels):\n",
    "    return softmax(logits) - labels\n",
    "\n",
    "\n",
    "# Mapping from activation functions to its derivatives.\n",
    "GRAD_DICT = {linear: d_linear, sigmoid: d_sigmoid, tanh: d_tanh, relu: d_relu}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the MLP, let's prepare the data. \n",
    "\n",
    "We will still use the MNIST dataset, but we will preprocess the data differently for the neural network.\n",
    "\n",
    "We will normalize the pixel values to be between 0 and 1 and use one-hot encoding for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Loading and preprocessing data')\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False, data_home='data')\n",
    "X = X / 255.0\n",
    "y = y.astype(int)\n",
    "\n",
    "(train_data, test_data, train_labels, test_labels) = train_test_split(X, y, random_state=0, test_size=0.7)\n",
    "\n",
    "def create_one_hot_labels(labels, dim=10):\n",
    "    one_hot_labels = np.zeros((labels.shape[0], dim))\n",
    "    for i in range(labels.shape[0]):\n",
    "        one_hot_labels[i][labels[i]] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "# Convert labels from integers to one-hot encodings\n",
    "test_labels = create_one_hot_labels(test_labels)\n",
    "train_labels = create_one_hot_labels(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3C.2 Training the Neural Network (10pt)\n",
    "\n",
    "**Report the training loss, training accuracy, test loss, and test accuracy for the default network architecture.**\n",
    "\n",
    "    ` MLP(784, 10, [16], [sigmoid])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print('Initializing neural network')\n",
    "model = MLP(784, 10, [16], [sigmoid])\n",
    "\n",
    "selected = np.random.randint(test_data.shape[0], size=100)\n",
    "true_labels = np.argmax(test_labels[selected], axis=1)\n",
    "preds_init = model.predict(test_data[selected])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start training')\n",
    "\n",
    "n_train = train_data.shape[0]\n",
    "n_epochs = 25\n",
    "batch_size = 100\n",
    "opt = GradientDescentOptimizer(0.01)\n",
    "\n",
    "train_loss_list = []\n",
    "train_accuracy_list = []\n",
    "for i in range(n_epochs):\n",
    "    sum_loss = 0.0\n",
    "    for j in range((n_train - 1) // batch_size + 1):\n",
    "        batch_data = train_data[j*batch_size:(j+1)*batch_size]\n",
    "        batch_labels = train_labels[j*batch_size:(j+1)*batch_size]\n",
    "        _, loss = model.forwardprop(batch_data, batch_labels)\n",
    "        if np.isnan(loss):\n",
    "            print('batch %s loss is abnormal')\n",
    "            print(loss)\n",
    "            continue\n",
    "        sum_loss += loss\n",
    "        model.backprop(batch_labels)\n",
    "        opt.update(model)\n",
    "    train_loss = sum_loss/(j+1)\n",
    "    train_accuracy = (np.sum(model.predict(train_data) == \n",
    "                              np.argmax(train_labels, axis=1)) / \n",
    "                      np.float64(train_labels.shape[0]))\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    print('=' * 20 + ('Epoch %d' % i) + '=' * 20)\n",
    "    print('Train loss %s accuracy %s' % (train_loss, train_accuracy))\n",
    "\n",
    "# Compute test loss and accuracy.\n",
    "_, test_loss = model.forwardprop(test_data, test_labels)\n",
    "test_accuracy = (np.sum(model.predict(test_data) == \n",
    "                        np.argmax(test_labels, axis=1)) / \n",
    "                  np.float64(test_labels.shape[0]))\n",
    "print('=' * 20 + 'Training finished' + '=' * 20 + '\\n')\n",
    "print ('Test loss %s accuracy %s\\n' %\n",
    "        (test_loss, test_accuracy))\n",
    "\n",
    "preds_trained = model.predict(test_data[selected])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training loss and accuracy as a function of the number of epochs for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training loss and accuracy separately.\n",
    "plt.plot(range(n_epochs), train_loss_list, label='train loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(n_epochs), train_accuracy_list, label='train accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3C.3 Visualizing the Predictions and Weights (5pt)\n",
    "\n",
    "In your solution, **include a plot of the test samples using the following code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot sample test images together with their groundtruth \n",
    "# and predicted labels before and after training\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "fig.subplots_adjust(wspace=0)\n",
    "for a, image, true_label, pred_init, pred_trained in zip(\n",
    "        axes.flatten(), test_data[selected],\n",
    "        true_labels, preds_init, preds_trained):\n",
    "    a.imshow(image.reshape(28, 28), cmap='gray_r')\n",
    "    a.text(0, 10, str(true_label), color=\"black\", size=15)\n",
    "    a.text(0, 26, str(pred_trained), color=\"blue\", size=15)\n",
    "    a.text(22, 26, str(pred_init), color=\"red\", size=15)\n",
    "\n",
    "    a.set_xticks(())\n",
    "    a.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please include a **visualization of the weights of the first layer in the MLP**, along with a **detailed explanation of the code** and **a brief summary of any observed patterns and understandings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "fig.subplots_adjust(wspace=0)\n",
    "idx = 0\n",
    "for a in axes.flatten():\n",
    "    a.imshow(model.layers[0].w[:, idx].reshape((28, 28)))\n",
    "    idx +=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3C.4: Hyperparameter tuning\n",
    "\n",
    "Modify only the network architecture (e.g. number of hidden units, activation functions, etc.) to improve the test accuracy, and provide a summary of your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print('Initializing neural network')\n",
    "\n",
    "model = None ## Your code here ##\n",
    "\n",
    "\n",
    "################# Do not modify the following code ############\n",
    "selected = np.random.randint(test_data.shape[0], size=100)\n",
    "true_labels = np.argmax(test_labels[selected], axis=1)\n",
    "preds_init = model.predict(test_data[selected])\n",
    "\n",
    "print('Start training')\n",
    "\n",
    "n_train = train_data.shape[0]\n",
    "n_epochs = 25\n",
    "batch_size = 100\n",
    "opt = GradientDescentOptimizer(0.01)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    sum_loss = 0.0\n",
    "    for j in range((n_train - 1) // batch_size + 1):\n",
    "        batch_data = train_data[j*batch_size:(j+1)*batch_size]\n",
    "        batch_labels = train_labels[j*batch_size:(j+1)*batch_size]\n",
    "        _, loss = model.forwardprop(batch_data, batch_labels)\n",
    "        if np.isnan(loss):\n",
    "            print('batch %s loss is abnormal')\n",
    "            print(loss)\n",
    "            continue\n",
    "        sum_loss += loss\n",
    "        model.backprop(batch_labels)\n",
    "        opt.update(model)\n",
    "    train_loss = sum_loss/(j+1)\n",
    "    train_accuracy = (np.sum(model.predict(train_data) == \n",
    "                              np.argmax(train_labels, axis=1)) / \n",
    "                      np.float64(train_labels.shape[0]))\n",
    "    \n",
    "    print('=' * 20 + ('Epoch %d' % i) + '=' * 20)\n",
    "    print('Train loss %s accuracy %s' % (train_loss, train_accuracy))\n",
    "\n",
    "# Compute test loss and accuracy.\n",
    "_, test_loss = model.forwardprop(test_data, test_labels)\n",
    "test_accuracy = (np.sum(model.predict(test_data) == \n",
    "                        np.argmax(test_labels, axis=1)) / \n",
    "                  np.float64(test_labels.shape[0]))\n",
    "print('=' * 20 + 'Training finished' + '=' * 20 + '\\n')\n",
    "print ('Test loss %s accuracy %s\\n' %\n",
    "        (test_loss, test_accuracy))\n",
    "\n",
    "preds_trained = model.predict(test_data[selected])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Bonus) Task 3C.5: Adding Momentum Optimization (10 pt)\n",
    "\n",
    "### Overview\n",
    "Gradient descent with momentum helps accelerate training by smoothing the update steps. Instead of updating weights solely based on the current gradient, momentum considers past updates to maintain velocity in the optimization path.\n",
    "\n",
    "### Why Using Momentum?\n",
    "- Helps navigate **flat regions** and **smoother convergence**.\n",
    "- Reduces **oscillations** in high-curvature areas.\n",
    "- Allows for **faster convergence** than standard gradient descent.\n",
    "\n",
    "### Momentum Update Rule\n",
    "For each weight $\\mathbf{w}$ and bias $b$, we maintain a **velocity term** $\\mathbf{v}$ to update parameters as follows:\n",
    "\n",
    "1. Compute the velocity update:\n",
    "   $$\n",
    "   \\mathbf{v} = \\beta \\mathbf{v} - \\eta \\nabla \\mathbf{w}\n",
    "   $$\n",
    "   where:\n",
    "   - $\\mathbf{v}$ is the velocity (momentum term),\n",
    "   - $\\beta$ is the momentum coefficient (e.g., 0.9),\n",
    "   - $\\eta$ is the learning rate,\n",
    "   - $\\nabla \\mathbf{w}$ is the current gradient.\n",
    "\n",
    "2. Update the parameters using the velocity:\n",
    "   $$\n",
    "   \\mathbf{w} = \\mathbf{w} + \\mathbf{v}\n",
    "   $$\n",
    "\n",
    "### Implementation Steps\n",
    "1. Modify the optimizer to **store velocity** for each parameter.\n",
    "2. Update parameters using both **past velocity** and **current gradients**.\n",
    "3. Compare training loss and accuracy **with and without momentum**.\n",
    "\n",
    "In the next code cell, you will implement the `MomentumOptimizer` class following the above steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MomentumOptimizer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        learning_rate: float, \n",
    "        momentum: float = 0.9, \n",
    "        decay_steps: int = 1000, \n",
    "        decay_rate: float = 1.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Stochastic Gradient Descent with Momentum.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): Initial learning rate.\n",
    "            momentum (float): Momentum factor (default: 0.9).\n",
    "            decay_steps (int): Number of steps after which learning rate decays (default: 1000).\n",
    "            decay_rate (float): Multiplicative factor for learning rate decay (default: 1.0).\n",
    "        \"\"\"\n",
    "        self.learning_rate: float = learning_rate\n",
    "        self.momentum: float = momentum\n",
    "        self.steps: int = 0\n",
    "        self.decay_steps: int = decay_steps\n",
    "        self.decay_rate: float = decay_rate\n",
    "        \n",
    "        # Initialize velocity dictionaries for weights and biases\n",
    "        self.velocity_w: dict[int, np.ndarray] = {}  \n",
    "        self.velocity_b: dict[int, np.ndarray] = {}  \n",
    "\n",
    "    def update(self, model) -> None:\n",
    "        \"\"\"\n",
    "        Update model parameters using momentum-based gradient descent.\n",
    "\n",
    "        Args:\n",
    "            model: The MLP model with layers containing weights (w), biases (b), \n",
    "                   and their corresponding gradients (d_w, d_b).\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if i not in self.velocity_w:\n",
    "                # Initialize velocity terms for this layer\n",
    "                self.velocity_w[i] = ### YOUR CODE HERE ###\n",
    "                self.velocity_b[i] = ### YOUR CODE HERE ###\n",
    "            \n",
    "            # Compute velocity updates\n",
    "            self.velocity_w[i] = ### YOUR CODE HERE ###\n",
    "            self.velocity_b[i] = ### YOUR CODE HERE ###\n",
    "            \n",
    "            # Apply updates to parameters\n",
    "            layer.w = ### YOUR CODE HERE ###\n",
    "            layer.b = ### YOUR CODE HERE ###\n",
    "        \n",
    "        # Apply learning rate decay\n",
    "        self.steps += 1\n",
    "        if self.steps % self.decay_steps == 0:\n",
    "            self.learning_rate = ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code cell, you will initialize a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print('Initializing neural network')\n",
    "model = MLP(784, 10, [16], [sigmoid])\n",
    "\n",
    "selected = np.random.randint(test_data.shape[0], size=100)\n",
    "true_labels = np.argmax(test_labels[selected], axis=1)\n",
    "preds_init = model.predict(test_data[selected])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code cell, you will use the function `train_model(optimizer)` to run training with your optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train model and return loss & accuracy\n",
    "def train_model(optimizer, n_epochs=25, batch_size=100):\n",
    "    model = MLP(784, 10, [16], [sigmoid])\n",
    "    opt = optimizer  # Use either GD or Momentum\n",
    "    optimizer_name = opt.__class__.__name__ \n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_accuracy_list = []\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        sum_loss = 0.0\n",
    "        for j in range((n_train - 1) // batch_size + 1):\n",
    "            batch_data = train_data[j * batch_size:(j + 1) * batch_size]\n",
    "            batch_labels = train_labels[j * batch_size:(j + 1) * batch_size]\n",
    "            \n",
    "            _, loss = model.forwardprop(batch_data, batch_labels)\n",
    "            if np.isnan(loss):\n",
    "                continue\n",
    "            \n",
    "            sum_loss += loss\n",
    "            model.backprop(batch_labels)\n",
    "            opt.update(model)\n",
    "\n",
    "        train_loss = sum_loss / (j + 1)\n",
    "        train_accuracy = (np.sum(model.predict(train_data) == np.argmax(train_labels, axis=1)) / \n",
    "                          np.float64(train_labels.shape[0]))\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "        print('=' * 20 + ('Epoch %d' % i) + '=' * 20)\n",
    "        print('Train loss %s accuracy %s' % (train_loss, train_accuracy))\n",
    "    \n",
    "    _, test_loss = model.forwardprop(test_data, test_labels)\n",
    "    test_accuracy = (np.sum(model.predict(test_data) == np.argmax(test_labels, axis=1)) / \n",
    "                      np.float64(test_labels.shape[0]))\n",
    "    print('=' * 20 + 'Training finished' + '=' * 20 + '\\n')\n",
    "    print ('Test loss %s accuracy %s\\n' %\n",
    "            (test_loss, test_accuracy))\n",
    "\n",
    "    return train_loss_list, train_accuracy_list, test_loss, test_accuracy, optimizer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with both optimizers\n",
    "print(\"Gradient Descent\")\n",
    "loss_gd, acc_gd, test_loss_gd, test_acc_gd, name_gd = train_model(GradientDescentOptimizer(0.01))\n",
    "\n",
    "print(\"Gradient Descent with Momentum\")\n",
    "loss_mo, acc_mo, test_loss_mo, test_acc_mo, name_mo = train_model(MomentumOptimizer(0.01, 0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, you will plot training loss over epochs for both optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss over epochs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "# --- PLOT FOR GD\n",
    "# --- PLOT FOR GD WITH MOMENTUM\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss vs. Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print final test accuracy\n",
    "print(f\"Test Accuracy ({name_gd}): {test_acc_gd:.4f}\")\n",
    "print(f\"Test Accuracy ({name_mo}): {test_acc_mo:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the parameters, then **answer the following questions**:\n",
    "1. Which optimizer converges faster? Justify your answer based on the training loss curves.\n",
    "2. Does momentum improve test accuracy? Why or why not?\n",
    "3. In what situations would momentum be more beneficial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You answer below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Bonus) Task 3C.6: Compare MLP to CNN (10pt)\n",
    "1. Complete an implmentation of convolutional neural network below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Assume these variables are already defined:\n",
    "#   train_data: NumPy array of shape (N_train, 784) with values in [0, 1]\n",
    "#   test_data:  NumPy array of shape (N_test, 784) with values in [0, 1]\n",
    "#   train_labels: one-hot encoded labels (shape: (N_train, 10))\n",
    "#   test_labels:  one-hot encoded labels (shape: (N_test, 10))\n",
    "\n",
    "# Convert one-hot encoded labels to integer labels\n",
    "train_labels_int = np.argmax(train_labels, axis=1)\n",
    "test_labels_int  = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Reshape the data from (N, 784) to (N, 1, 28, 28)\n",
    "train_data_tensor = torch.from_numpy(train_data.reshape(-1, 1, 28, 28)).float()\n",
    "test_data_tensor  = torch.from_numpy(test_data.reshape(-1, 1, 28, 28)).float()\n",
    "\n",
    "# Convert labels to PyTorch tensors\n",
    "train_labels_tensor = torch.from_numpy(train_labels_int).long()\n",
    "test_labels_tensor  = torch.from_numpy(test_labels_int).long()\n",
    "\n",
    "# Create DataLoaders for training and testing\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Define a simple CNN model for MNIST.\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # First convolutional block: Conv -> ReLU -> MaxPool\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)  # TODO: Adjust parameters if needed\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second convolutional block: Conv -> ReLU -> MaxPool\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)  # TODO: Adjust parameters if needed\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # After two pooling layers, the 28x28 image becomes 7x7 feature maps.\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # TODO: Ensure dimensions match\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 classes for MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # TODO: Flatten the tensor\n",
    "        x = self.fc2(self.relu3(self.fc1(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the CNN, define loss and optimizer.\n",
    "\n",
    "# TODO: Instantiate the CNN model\n",
    "cnn_model = ### YOUR CODE HERE ###\n",
    "\n",
    "# TODO: Define the loss function\n",
    "lossfunc = ### YOUR CODE HERE ###\n",
    "\n",
    "# TODO: Set up the optimizer\n",
    "optimizer = ### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5\n",
    "cnn_model.train()  # Set the model to training mode\n",
    "print(\"Training CNN model...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()         # Clear the gradients\n",
    "        outputs = cnn_model(images)     # Forward pass\n",
    "        loss = lossfunc(outputs, labels)\n",
    "        loss.backward()               # Backward pass\n",
    "        optimizer.step()              # Update parameters\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"CNN training finished!\")\n",
    "cnn_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "# Evaluate the CNN on the test set.\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = cnn_model(images)\n",
    "# TODO: Extract predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# print test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for you to draw your own number and predict the outcome.\n",
    "\n",
    "**Question**: Brifely talk about which method for the task of handwritten text reconigtion is better and why do you think that is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "\n",
    "class DigitDrawer:\n",
    "    def __init__(self, master, nn_model, cnn_model):\n",
    "        self.master = master\n",
    "        self.master.title(\"Digit Recognition\")\n",
    "\n",
    "        self.canvas_width = 280\n",
    "        self.canvas_height = 280\n",
    "        self.canvas = tk.Canvas(master, width=self.canvas_width, height=self.canvas_height, bg=\"white\")\n",
    "        self.canvas.pack(padx=10, pady=10)\n",
    "\n",
    "        # Frame for buttons\n",
    "        button_frame = tk.Frame(master)\n",
    "        button_frame.pack(pady=5)\n",
    "        \n",
    "        self.button_predict = tk.Button(button_frame, text=\"Predict\", command=self.predict)\n",
    "        self.button_predict.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.button_clear = tk.Button(button_frame, text=\"Clear\", command=self.clear)\n",
    "        self.button_clear.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        # Create a high-res image for smoother drawing.\n",
    "        # We later downsample to 28x28 for the model.\n",
    "        self.image = Image.new(\"L\", (self.canvas_width, self.canvas_height), 255)\n",
    "        self.draw = ImageDraw.Draw(self.image)\n",
    "\n",
    "        self.canvas.bind(\"<B1-Motion>\", self.paint)\n",
    "\n",
    "        self.nn_model = nn_model\n",
    "        self.cnn_model = cnn_model\n",
    "        \n",
    "    def softmax(x):\n",
    "        # x is assumed to be a NumPy array\n",
    "        shifted_x = x - np.max(x, axis=1, keepdims=True)  \n",
    "        exp_x = np.exp(shifted_x)\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "    def paint(self, event):\n",
    "        \"\"\"Draw on both the canvas and the high-res PIL image.\"\"\"\n",
    "        # Define the brush radius\n",
    "        radius = 8  \n",
    "        x, y = event.x, event.y\n",
    "        x1, y1 = x - radius, y - radius\n",
    "        x2, y2 = x + radius, y + radius\n",
    "\n",
    "        # Draw on the Tkinter canvas (for display)\n",
    "        self.canvas.create_oval(x1, y1, x2, y2, fill=\"black\", outline=\"black\")\n",
    "        # Draw on the high resolution PIL image (for prediction)\n",
    "        self.draw.ellipse([x1, y1, x2, y2], fill=\"black\")\n",
    "\n",
    "    def preprocess_image(self):\n",
    "        \"\"\"\n",
    "        Downsample the high-res image (280x280) to 28x28 using a high-quality\n",
    "        resampling filter (LANCZOS) to preserve the drawing's smoothness.\n",
    "        Then normalize the pixel values to [0,1].\n",
    "        \"\"\"\n",
    "        img_resized = self.image.resize((28, 28), resample=Image.LANCZOS)\n",
    "        # Convert image to numpy array and normalize (white background=1, black=0)\n",
    "        img_array = np.array(img_resized).reshape(1, 1, 28, 28) / 255.0\n",
    "        return img_array\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"Predict the digit using both the NN and CNN models and display the results.\"\"\"\n",
    "        img_array = self.preprocess_image()\n",
    "\n",
    "        # --- NN Prediction ---\n",
    "        # Use the NN's forward propagation method to get full probabilities.\n",
    "        nn_probs, _ = self.nn_model.forwardprop(img_array.reshape(1, -1))\n",
    "        nn_probs = nn_probs[0]  # probabilities for the single sample\n",
    "        nn_prediction = np.argmax(nn_probs)\n",
    "\n",
    "        # --- CNN Prediction ---\n",
    "        self.cnn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            img_tensor = torch.tensor(img_array, dtype=torch.float32)\n",
    "            cnn_output = self.cnn_model(img_tensor)\n",
    "            cnn_probs = torch.nn.functional.softmax(cnn_output, dim=1).numpy()[0]\n",
    "            cnn_prediction = np.argmax(cnn_probs)\n",
    "\n",
    "        # Create a figure to display the drawn image and the prediction probabilities\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        # Left subplot: the high-res drawn digit (before downsampling)\n",
    "        axes[0].imshow(self.image, cmap='gray')\n",
    "        axes[0].set_title(\"Drawn Digit\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # Right subplot: a bar chart comparing prediction probabilities\n",
    "        digits = np.arange(10)\n",
    "        bar_width = 0.35\n",
    "        axes[1].bar(digits - bar_width/2, nn_probs, bar_width, label='NN', color='skyblue')\n",
    "        axes[1].bar(digits + bar_width/2, cnn_probs, bar_width, label='CNN', color='salmon')\n",
    "        axes[1].set_xticks(digits)\n",
    "        axes[1].set_xlabel(\"Digit\")\n",
    "        axes[1].set_ylabel(\"Probability\")\n",
    "        axes[1].set_title(\"Prediction Probabilities\")\n",
    "        axes[1].legend()\n",
    "\n",
    "        plt.suptitle(f\"Predictions - NN: {nn_prediction}, CNN: {cnn_prediction}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Clear the canvas and reset the high-res image.\"\"\"\n",
    "        self.canvas.delete(\"all\")\n",
    "        self.image = Image.new(\"L\", (self.canvas_width, self.canvas_height), 255)\n",
    "        self.draw = ImageDraw.Draw(self.image)\n",
    "\n",
    "# --------------------------\n",
    "# Launch the Graphical User Interface\n",
    "# --------------------------\n",
    "# Ensure that 'model' (the NN) and 'cnn_model' (the CNN) are loaded before this code runs.\n",
    "root = tk.Tk()\n",
    "app = DigitDrawer(root, model, cnn_model)\n",
    "root.mainloop()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

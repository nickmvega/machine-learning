{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Report - Nick Vega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will use a perceptron to deal with a 2D classification problem.\n",
    "\n",
    "Code was provided for generating the training set, test set, and plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Given Data](https://raw.githubusercontent.com/nickmvega/machine-learning/651cd6232136ddccbf6935ff38af8827c4095c44/images_p2/given_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2A.1 Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to implement a binary classifier with the Perceptron class.\n",
    "\n",
    "`class Perceptron(object)`\n",
    "\n",
    "The methods in this class include the following:\n",
    "- `__init__(self, T=1)`: In this method, the number of iterations T is defined.\n",
    "\n",
    "- `fit(self, X, y)`: In this method, we train the perceptron model on data X with labels y and iteration T. We are given initailization code for the number of samples, the number of features, a weight vector, and a bias variabel. In the code we implemented, we conduct an iteration up to the number of iterations T. Within each iteration, we conduct an iteration up to the number of samples. In the inner iterations, we check if the predicted class for X[i] is not equal to y[i]. If this is true we add y[i] * X[i] to the weight and y[i] to the bias. \n",
    "\n",
    "- `project(self, X)`: In this method, we project data X onto the learned hyperplane with weights w and bias b. The code works by computing the dot product of X and the weight vector and adding the result to the bias vector.\n",
    "\n",
    "- `predict(self, X)`: In this method, we predict class labels for samples in X using the project method defined above. The code works by calling the project function above which returns a projection vector. We then loop through each element in the vector and if the result is greater than or equal to 0, we set our prediciton to be 1, otherwise we set our prediction to be -1. We then return our predictions. \n",
    "\n",
    "\n",
    "Using the Perceptron class we have defined above, we set T=5 and fit the model using the given Xtrain and ytrain sets. We then predict on the given Xtest. We achieve an accuracy of 100.00% and plot the decision boundary below\n",
    "\n",
    "![Initial Perceptron](https://raw.githubusercontent.com/nickmvega/machine-learning/651cd6232136ddccbf6935ff38af8827c4095c44/images_p2/initial_perceptron.png)\n",
    "\n",
    "\n",
    "This dataset is linearly separable and we are able to classify the data with 100% accuracy given the testing dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2A.2 Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will build kernel functions and a kernel perceptron. We will then plot the decision boundary using the kernel perceptron class we created and for their corresponding kernel functions.\n",
    "\n",
    "The decision function for the Kernel Perceptron is given by \n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\text{sign}\\left(\\sum_{i=1}^{n} \\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{x})\\right)\n",
    "$$\n",
    "\n",
    "where k is the kernel function y_i are the labels and alpha_i are the learend weights. \n",
    "\n",
    "The kernel (Gram) matrix induced by the kernel function k over n data points is defined as \n",
    "\n",
    "$$\n",
    "\\mathbf{K}=\n",
    "\\left(\\begin{array}{ccc} \n",
    "k(\\mathbf{x}_1,\\mathbf{x}_1) & \\dots & k(\\mathbf{x}_1,\\mathbf{x}_n)\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "k(\\mathbf{x}_n,\\mathbf{x}_1) & \\dots & k(\\mathbf{x}_n,\\mathbf{x}_n)\n",
    "\\end{array}\\right)\n",
    "$$ \n",
    "\n",
    "Given a test data point **x**, the predicted label is\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{sign}\\left(\\sum_{i=1}^{n} \\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{x})\\right)\n",
    "$$\n",
    "\n",
    "We are given the following generated dataset below:\n",
    "\n",
    "![Given Data 2](https://raw.githubusercontent.com/nickmvega/machine-learning/651cd6232136ddccbf6935ff38af8827c4095c44/images_p2/given_data_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KernelPerceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first created a Kernel Perceptron class described below:\n",
    "\n",
    "`class KernelPerceptron(object)`\n",
    "\n",
    "The methods in this class include the following:\n",
    "\n",
    "`__init__(self, kernel=PolynomialKernel(p = 1), T=1)`: In this method, we define a kernel, the number of iterations T, alpha, Xtrain, ytrain. \n",
    "\n",
    "`fit(self, X, y)`: In this method, we fill the Gram matrix defind below\n",
    "$$\n",
    "\\mathbf{K}=\n",
    "\\left(\\begin{array}{ccc} \n",
    "k(\\mathbf{x}_1,\\mathbf{x}_1) & \\dots & k(\\mathbf{x}_1,\\mathbf{x}_n)\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "k(\\mathbf{x}_n,\\mathbf{x}_1) & \\dots & k(\\mathbf{x}_n,\\mathbf{x}_n)\n",
    "\\end{array}\\right)\n",
    "$$ \n",
    "by calling the kernel objected that is passed through the initialization of the KernelPerceptron. \n",
    "\n",
    "We then set the alpha values using one outer iteration up to the number of iterations T and one inner iteration up the number of samples. We then check if the function defined below \n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\text{sign}\\left(\\sum_{i=1}^{n} \\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{x})\\right)\n",
    "$$\n",
    "\n",
    "is not equal to y[i]. If this is the case, we add 1 to alpha[i].\n",
    "\n",
    "`project(self, X)`: In this method, we are creating a vector that stores the following result\n",
    "\n",
    "$$\n",
    "\\text{projection} = \\left(\\sum_{i=1}^{n} \\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{x})\\right)\n",
    "$$\n",
    "\n",
    "In the code, we achieve this using two iterations, the outer iteration being up to the number of rows in X and the inner iteration being up the number of rows in Xtrain. In each inner iteration we then compute the following \n",
    "\n",
    "$$\n",
    "\\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{x})\n",
    "$$\n",
    "\n",
    "and add the result to projection[i].\n",
    "\n",
    "`predict(self, X)`: In this method, we call the project method we have just described above with X. We then take the resulting projection vector and iterate through each element. For each element, we check if the projection of that element is greater than or equal to 0 and if so set the prediction for this element in our y_hat vector to be 1. Otherwise, we set the prediction for this element in our y_hat vector to be -1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create three Kernel functions for a `PolynomialKernel`, `GaussianKernel`, and `LaplaceKernel`. Each KernelFunction and their methods are defined below.\n",
    "\n",
    "`PolynomialKernel`. The polynomial kernel function has two methods:\n",
    "\n",
    "`__init__(self, p=1)`: In this method, p is the degree of the polynomial and set to 1 when not specified\n",
    "\n",
    "`__call__(self, x, y)`: In this method, we implement the polynomial kernel function which is defined below\n",
    "\n",
    "$$\n",
    "k_{\\text{poly}}(\\mathbf{x},\\mathbf{x}', d) = (1+\\mathbf{x}^\\top \\mathbf{x}')^d\n",
    "$$\n",
    "\n",
    "In the code, the dot product of x transpose and y is added to 1, and then set to the pth exponent.\n",
    "\n",
    "`GaussianKernel`. The gaussian kernel function has two methods:\n",
    "\n",
    "`__init__(self, sigma=1)`: In this method, sigma is an important parameter in the RBF kernel function and is set to 5 as default. \n",
    "\n",
    "`__call__(self, x, y)`: In this method, we implement the gaussian kernel function which is defined below \n",
    "\n",
    "$$\n",
    "k_{\\text{RBF}}(\\mathbf{x},\\mathbf{x}', \\sigma) = \\exp\\left(-\\frac{\\lVert \\mathbf{x}-\\mathbf{x'} \\rVert^2_2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "In the code, np.linalg.norm is used to subtract x and y. The norm is then divided by the denominator of 2 * sigma ** 2. Then, np.exp of the negative result is returned.\n",
    "\n",
    "`LaplaceKernel`. The Laplace Kernel function has two methods: \n",
    "\n",
    "`__init__(self, sigma=1)`: In this method, sigma is an important parameter in the laplace kernel function and is set to 5 as default. \n",
    "\n",
    "`__call__(self, x, y)`: In this method, we implement the laplace kernel function which is defined below \n",
    "\n",
    "$$\n",
    "k_{\\text{laplace}}(\\mathbf{x},\\mathbf{x}', \\sigma) = \\exp\\left(-\\frac{\\lVert \\mathbf{x}-\\mathbf{x'} \\rVert _1}{\\sigma}\\right)\n",
    "$$\n",
    "\n",
    "In the code, np.linalg.norm with ord=1 is used to subtract x and y. Then the norm is divided by sigma, and then np.exp of the negative result is returned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Perceptron Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is the data linearly separable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will know check if the given data is linearly separable by calling the KernelPerceptron class with the PolynomialKernel function where p=1. We train the KernelPerceptron using Xtrain and ytrain and predicton on Xtest.\n",
    "\n",
    "Our model achieved an accuracy of :\n",
    "- Accuracy: 47.50%\n",
    "\n",
    "We then plot the decision boundary on the dataset as seen below\n",
    "\n",
    "![Is Lin Sep?](https://raw.githubusercontent.com/nickmvega/machine-learning/35e74c44e8e9f45b2b819425ddfc9a6fe8831522/images_p2/is%20lin%20sep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More powerful kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform regression with the Polynomial Kernel, Gaussian Kernel, and Laplace Kernel functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Kernel**\n",
    "\n",
    "We performed regression with the Kernel Perceptron class given a Polynomial Kernel and varied the degree and number of iterations. The accuracy results were the following:\n",
    "\n",
    "\n",
    "- Accuracy with 2 epochs and degree 2 : 47.50%\n",
    "- Accuracy with 2 epochs and degree 3 : 97.50%\n",
    "- Accuracy with 2 epochs and degree 4 : 95.00%\n",
    "\n",
    "- Accuracy with 4 epochs and degree 2 : 57.50%\n",
    "- Accuracy with 4 epochs and degree 3 : 97.50%\n",
    "- Accuracy with 4 epochs and degree 4 : 95.00%\n",
    "\n",
    "- Accuracy with 6 epochs and degree 2 : 95.00%\n",
    "- Accuracy with 6 epochs and degree 3 : 97.50%\n",
    "- Accuracy with 6 epochs and degree 4 : 92.50%\n",
    "\n",
    "- Accuracy with 8 epochs and degree 2 : 97.50%\n",
    "- Accuracy with 8 epochs and degree 3 : 97.50%\n",
    "- Accuracy with 8 epochs and degree 4 : 97.50%\n",
    "\n",
    "- Accuracy with 10 epochs and degree 2 : 92.50%\n",
    "- Accuracy with 10 epochs and degree 3 : 97.50%\n",
    "- Accuracy with 10 epochs and degree 4 : 97.50%\n",
    "\n",
    "For a degree of 2, it takes about 8 epochs for the accuracy to plateau at 97.50%.\n",
    "\n",
    "For a degree of 3, it takes about 2 epochs for the accuracy to plateau at 97.50%.\n",
    "\n",
    "For a degree of 4 it takes about 8 epochs for the accuracy to plateau at 97.50%.\n",
    "\n",
    "Each degree of 2, 3 and 4 were able to achieve an accuracy 97.50% given a certain number of epochs. An example of a decision boundary for a polynomial kernel with p=3 and epochs=10 is seen below.\n",
    "\n",
    "![Poly P3](https://raw.githubusercontent.com/nickmvega/machine-learning/35e74c44e8e9f45b2b819425ddfc9a6fe8831522/images_p2/poly%20p3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Kernel**\n",
    "\n",
    "We performed regression with the Kernel Perceptron class given a Gaussian Kernel and varied the sigma and number of epochs. The accuracy results were the following:\n",
    "\n",
    "- Accuracy with 2 epochs and sigma 1 : 97.50%\n",
    "- Accuracy with 2 epochs and sigma 2 : 97.50%\n",
    "- Accuracy with 2 epochs and sigma 3 : 97.50%\n",
    "- Accuracy with 2 epochs and sigma 4 : 97.50%\n",
    "\n",
    "- Accuracy with 4 epochs and sigma 1 : 95.00%\n",
    "- Accuracy with 4 epochs and sigma 2 : 97.50%\n",
    "- Accuracy with 4 epochs and sigma 3 : 97.50%\n",
    "- Accuracy with 4 epochs and sigma 4 : 97.50%\n",
    "\n",
    "- Accuracy with 6 epochs and sigma 1 : 97.50%\n",
    "- Accuracy with 6 epochs and sigma 2 : 97.50%\n",
    "- Accuracy with 6 epochs and sigma 3 : 97.50%\n",
    "- Accuracy with 6 epochs and sigma 4 : 97.50%\n",
    "\n",
    "- Accuracy with 8 epochs and sigma 1 : 97.50%\n",
    "- Accuracy with 8 epochs and sigma 2 : 97.50%\n",
    "- Accuracy with 8 epochs and sigma 3 : 97.50%\n",
    "- Accuracy with 8 epochs and sigma 4 : 97.50%\n",
    "\n",
    "- Accuracy with 10 epochs and sigma 1 : 97.50%\n",
    "- Accuracy with 10 epochs and sigma 2 : 97.50%\n",
    "- Accuracy with 10 epochs and sigma 3 : 97.50%\n",
    "- Accuracy with 10 epochs and sigma 4 : 97.50%\n",
    "\n",
    "For a sigma of 1, it takes about 6 epochs for the accuracy to plateau at 97.50%.\n",
    "\n",
    "For a sigma of 2, it takes about 2 epochs for the accuracy to plateau at 97.50%.\n",
    "\n",
    "For a sigma of 3 it takes about 2 epochs for the accuracy to plateau at 97.50%.\n",
    "\n",
    "For a sigma of 4 it takes about 2 epochs for the accuracy to plateau at 97.50%.\n",
    "\n",
    "Each sigma of 1, 2, 3 and 4 were able to achieve an accuracy 97.50% given a certain number of epochs. An example of a decision boundary for a polynomial kernel with sigma=3 and epochs=10 is seen below.\n",
    "\n",
    "![Gaus S3]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Laplace Kernel**\n",
    "\n",
    "We performed regression with the Kernel Perceptron class given a Laplace Kernel and varied the sigma and number of epochs. The accuracy results were the following:\n",
    "\n",
    "- Accuracy with 2 epochs and sigma 1 : 95.00%\n",
    "- Accuracy with 2 epochs and sigma 3 : 95.00%\n",
    "- Accuracy with 2 epochs and sigma 5 : 95.00%\n",
    "\n",
    "- Accuracy with 4 epochs and sigma 1 : 97.50%\n",
    "- Accuracy with 4 epochs and sigma 3 : 95.00%\n",
    "- Accuracy with 4 epochs and sigma 5 : 95.00%\n",
    "\n",
    "- Accuracy with 6 epochs and sigma 1 : 97.50%\n",
    "- Accuracy with 6 epochs and sigma 3 : 95.00%\n",
    "- Accuracy with 6 epochs and sigma 5 : 47.50%\n",
    "\n",
    "- Accuracy with 8 epochs and sigma 1 : 97.50%\n",
    "- Accuracy with 8 epochs and sigma 3 : 95.00%\n",
    "- Accuracy with 8 epochs and sigma 5 : 95.00%\n",
    "\n",
    "- Accuracy with 10 epochs and sigma 1 : 97.50%\n",
    "- Accuracy with 10 epochs and sigma 3 : 95.00%\n",
    "- Accuracy with 10 epochs and sigma 5 : 95.00%\n",
    "\n",
    "For a sigma of 1, it takes about 4 epochs for the accuracy to plateau at 97.50%.\n",
    "\n",
    "For a sigma of 3, it takes about 2 epochs for the accuracy to plateau at 95.00%.\n",
    "\n",
    "For a sigma of 5 it takes about 8 epochs for the accuracy to plateau at 95.00%.\n",
    "\n",
    "Sigma=1 was able to achieve an accuracy 97.50% given a certain number of epochs. Sigma=3,5 were able to able to achieve an accuracy of only 95%. This means that smaller sigma values than 1 would have proved to be more accurate as the number of epochs increases.\n",
    "\n",
    "An example of a decision boundary for a polynomial kernel with sigma=1 and epochs=10 is seen below.\n",
    "\n",
    "![Laplace S1]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2B: Real-World Data Analysis: Seoul Bike Rental Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will analyze the SeoulBikeData.csv dataset, which provides information about bike rentals in Seoul. The dataset includes:\n",
    "- **6 Features**: Weather-related conditions like temperature, humidity, and wind speed.\n",
    "- **1 Time Feature**: Hour of the day.\n",
    "- **Target**: The number of rented bikes, with the objective of predicting whether `Rented Bike Count > 500`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Load and Explore the Dataset**:\n",
    "   - Load the `SeoulBikeData.csv` file using `pandas`.\n",
    "   - Display descriptive statistics and visualize feature distributions (e.g., histograms, pair plots).\n",
    "\n",
    "\n",
    "In this step, we loaded the SeoulBikeData.csv file using pandas and given code to binarize y and conduct a train test split. \n",
    "\n",
    "We display descriptive statistics and visualize feature distributions below:\n",
    "\n",
    "**DF Sample**\n",
    "       Rented Bike Count  Hour  Temperature (deg C)  Humidity(%)  \\\n",
    "8673                800     9                  5.0           75   \n",
    "7244                  0    20                 18.0           67   \n",
    "4194               2825    18                 22.6           39   \n",
    "5727                776    15                 35.3           52   \n",
    "7083                458     3                 18.8           90   \n",
    "\n",
    "      Wind speed (m/s)  Visibility (10m)  Dew point temperature (deg C)\n",
    "8673               0.3               390                            0.9   \n",
    "7244               0.7              2000                           11.7   \n",
    "4194               2.8              1655                            7.9   \n",
    "5727               1.9               822                           23.9   \n",
    "7083               1.0              1191                           17.1   \n",
    "\n",
    "      Solar Radiation (MJ/m2)  Rainfall(mm)  Snowfall (cm)  \n",
    "8673                     0.12           0.0            0.0  \n",
    "7244                     0.00           0.0            0.0  \n",
    "4194                     1.10           0.0            0.0  \n",
    "5727                     2.15           0.0            0.0  \n",
    "7083                     0.00           0.0            0.0   \n",
    "\n",
    "**DF description**\n",
    "        Rented Bike Count         Hour  Temperature (deg C)  Humidity\n",
    "count         1000.00000  1000.000000          1000.000000  1000.000000   \n",
    "mean           737.44300    11.424000            13.256600    58.108000   \n",
    "std            669.37817     6.886774            12.066944    19.731439   \n",
    "min              0.00000     0.000000           -16.200000     0.000000   \n",
    "25%            191.00000     5.000000             3.850000    44.000000   \n",
    "50%            552.00000    12.000000            14.100000    57.000000   \n",
    "75%           1118.75000    17.250000            23.325000    73.000000   \n",
    "max           3556.00000    23.000000            37.900000    98.000000   \n",
    "\n",
    "       Wind speed (m/s)  Visibility (10m)  Dew point temperature (degC) \n",
    "count        1000.00000       1000.000000                   1000.000000   \n",
    "mean            1.72070       1457.935000                       4.439100   \n",
    "std             1.03171        594.772765                      13.109437   \n",
    "min             0.00000         54.000000                     -30.500000   \n",
    "25%             0.90000        977.000000                      -4.600000   \n",
    "50%             1.50000       1717.000000                       5.300000   \n",
    "75%             2.40000       2000.000000                      15.700000   \n",
    "max             6.90000       2000.000000                      26.100000   \n",
    "\n",
    "       Solar Radiation (MJ/m2)  Rainfall(mm)  Snowfall (cm)  \n",
    "count              1000.000000   1000.000000    1000.000000  \n",
    "mean                  0.573570      0.114600       0.071400  \n",
    "std                   0.853094      0.846501       0.405939  \n",
    "min                   0.000000      0.000000       0.000000  \n",
    "25%                   0.000000      0.000000       0.000000  \n",
    "50%                   0.030000      0.000000       0.000000  \n",
    "75%                   0.940000      0.000000       0.000000  \n",
    "max                   3.490000     15.500000       5.100000   \n",
    "\n",
    "Feature Histograms:  \n",
    "\n",
    "![Feature Histograms]()\n",
    "\n",
    "Correlation Matrix:\n",
    "\n",
    "![Correlation Matrix]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Preprocessing**:\n",
    "   - Convert `Rented Bike Count` into a binary target (`1` if > 500, else `0`).\n",
    "   - Normalize the numerical features using min-max scaling or standardization.\n",
    "\n",
    "In this step, we convert the Rented Bike COunt into a binary target shown below:\n",
    "\n",
    "      Rented Bike Count  Hour  Temperature (deg C)  Humidity(%)  \\\n",
    "8673                  1     9                  5.0           75   \n",
    "7244                  0    20                 18.0           67   \n",
    "4194                  1    18                 22.6           39   \n",
    "5727                  1    15                 35.3           52   \n",
    "7083                  0     3                 18.8           90   \n",
    "\n",
    "      Wind speed (m/s)  Visibility (10m)  Dew point temperature (deg C)  \\\n",
    "8673               0.3               390                            0.9   \n",
    "7244               0.7              2000                           11.7   \n",
    "4194               2.8              1655                            7.9   \n",
    "5727               1.9               822                           23.9   \n",
    "7083               1.0              1191                           17.1   \n",
    "\n",
    "      Solar Radiation (MJ/m2)  Rainfall(mm)  Snowfall (cm)  \n",
    "8673                     0.12           0.0            0.0  \n",
    "7244                     0.00           0.0            0.0  \n",
    "4194                     1.10           0.0            0.0  \n",
    "5727                     2.15           0.0            0.0  \n",
    "7083                     0.00           0.0            0.0  \n",
    "\n",
    "We also normalize the numerical features using min-max scaling shown below: \n",
    "\n",
    "      Rented Bike Count      Hour  Temperature (deg C)  Humidity(%)  \\\n",
    "8673                1.0  0.391304             0.391867     0.765306   \n",
    "7244                0.0  0.869565             0.632163     0.683673   \n",
    "4194                1.0  0.782609             0.717190     0.397959   \n",
    "5727                1.0  0.652174             0.951941     0.530612   \n",
    "7083                0.0  0.130435             0.646950     0.918367   \n",
    "\n",
    "      Wind speed (m/s)  Visibility (10m)  Dew point temperature (deg C)  \\\n",
    "8673          0.043478          0.172662                       0.554770   \n",
    "7244          0.101449          1.000000                       0.745583   \n",
    "4194          0.405797          0.822713                       0.678445   \n",
    "5727          0.275362          0.394656                       0.961131   \n",
    "7083          0.144928          0.584275                       0.840989   \n",
    "\n",
    "      Solar Radiation (MJ/m2)  Rainfall(mm)  Snowfall (cm)  \n",
    "8673                 0.034384           0.0            0.0  \n",
    "7244                 0.000000           0.0            0.0  \n",
    "4194                 0.315186           0.0            0.0  \n",
    "5727                 0.616046           0.0            0.0  \n",
    "7083                 0.000000           0.0            0.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Kernel-Based Modeling**:\n",
    "\n",
    "In this step, we find a proper kernel function to solve all classifciaton tasks.\n",
    "\n",
    "We identify and implement a sigmoid kernel function defined below:\n",
    "\n",
    "$$\n",
    "k_{\\text{sigmoid}}(\\mathbf{x}, \\mathbf{x}', \\alpha, c) = \\tanh(\\alpha \\cdot \\mathbf{x}^\\top \\mathbf{x}' + c)\n",
    "$$\n",
    "\n",
    "`NewKernel(object)` This Kernel class represents a sigmoid kernel function. The following class has two methods:\n",
    "- `__init__(self, alpha=1, c=0)`: In this method, we initalize alpha as default 1 and c as default 0. These are important parameters in the sigmoid kernel function as seen above. \n",
    "- `__call__(self, x, y)`: In this method, we implement the sigmoid kernel function above using np.tanh and multiplying alpha and the dot product of x tranpose and y and then adding c all within the tanh function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now optimize the hyperparameters of each Kernel, namely the Polynomial Kernel, Gaussian Kernel, Laplace Kernel, and Sigmoid Kernel functions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Evaluation and Analysis**:\n",
    "- Compare the performance of different kernels using accuracy and\n",
    "classification reports.\n",
    "- Visualize decision boundaries for the Laplace kernel with the optimal $\\sigma$ using principle component analysis (PCA) to project data in higher dimensional feature space to 3 dimensions. Then plot the decision boundary in the same graph to visualize. Assuming the features are linearly independent, it would take the full dimension of the feature to capture 100 percent of the variation, however if we assume the data is of low \"numerical rank\", plotting the first say 3 dominant dimension of the feature will give a good representation of the data.\n",
    "- Use `pca_3d = PCA(n_components=3); X_pca_3d = pca_3d.fit_transform(Xtest)` followed by `pca_3d.explained_variance_ratio_` and `np.sum(explained_variance_ratio) * 100:.2f}%` to see how much of the variance is explained by 3 dominant principle component."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

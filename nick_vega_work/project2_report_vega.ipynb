{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Report - Nick Vega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will use a perceptron to deal with a 2D classification problem.\n",
    "\n",
    "Code was provided for generating the training set, test set, and plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Given Data]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2A.1 Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to implement a binary classifier with the Perceptron class.\n",
    "\n",
    "`class Perceptron(object)`\n",
    "\n",
    "The methods in this class include the following:\n",
    "- `__init__(self, T=1)`: In this method, the number of iterations T is defined.\n",
    "\n",
    "- `fit(self, X, y)`: In this method, we train the perceptron model on data X with labels y and iteration T. We are given initailization code for the number of samples, the number of features, a weight vector, and a bias variabel. In the code we implemented, we conduct an iteration up to the number of iterations T. Within each iteration, we conduct an iteration up to the number of samples. In the inner iterations, we check if the predicted class for X[i] is not equal to y[i]. If this is true we add y[i] * X[i] to the weight and y[i] to the bias. \n",
    "\n",
    "- `project(self, X)`: In this method, we project data X onto the learned hyperplane with weights w and bias b. The code works by computing the dot product of X and the weight vector and adding the result to the bias vector.\n",
    "\n",
    "- `predict(self, X)`: In this method, we predict class labels for samples in X using the project method defined above. The code works by calling the project function above which returns a projection vector. We then loop through each element in the vector and if the result is greater than or equal to 0, we set our prediciton to be 1, otherwise we set our prediction to be -1. We then return our predictions. \n",
    "\n",
    "\n",
    "Using the Perceptron class we have defined above, we set T=5 and fit the model using the given Xtrain and ytrain sets. We then predict on the given Xtest. We achieve an accuracy of 100.00% and plot the decision boundary below\n",
    "\n",
    "![Initial Perceptron]()\n",
    "\n",
    "\n",
    "This dataset is linearly separable and we are able to classify the data with 100% accuracy given the testing dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2A.2 Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will build kernel functions and a kernel perceptron. We will then plot the decision boundary using the kernel perceptron class we created and for their corresponding kernel functions.\n",
    "\n",
    "The decision function for the Kernel Perceptron is given by \n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\text{sign}\\left(\\sum_{i=1}^{n} \\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{x})\\right)\n",
    "$$\n",
    "\n",
    "where k is the kernel function y_i are the labels and alpha_i are the learend weights. \n",
    "\n",
    "The kernel (Gram) matrix induced by the kernel function k over n data points is defined as \n",
    "\n",
    "$$\n",
    "\\mathbf{K}=\n",
    "\\left(\\begin{array}{ccc} \n",
    "k(\\mathbf{x}_1,\\mathbf{x}_1) & \\dots & k(\\mathbf{x}_1,\\mathbf{x}_n)\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "k(\\mathbf{x}_n,\\mathbf{x}_1) & \\dots & k(\\mathbf{x}_n,\\mathbf{x}_n)\n",
    "\\end{array}\\right)\n",
    "$$ \n",
    "\n",
    "Given a test data point **x**, the predicted label is\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{sign}\\left(\\sum_{i=1}^{n} \\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{x})\\right)\n",
    "$$\n",
    "\n",
    "We are given the following generated dataset below:\n",
    "\n",
    "![Given Data 2]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KernelPerceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first created a Kernel Perceptron class described below:\n",
    "\n",
    "`class KernelPerceptron(object)`\n",
    "\n",
    "The methods in this class include the following:\n",
    "\n",
    "`__init__(self, kernel=PolynomialKernel(p = 1), T=1)`: In this method, we define a kernel, the number of iterations T, alpha, Xtrain, ytrain. \n",
    "\n",
    "`fit(self, X, y)`: In this method, we fill the Gram matrix defind below\n",
    "$$\n",
    "\\mathbf{K}=\n",
    "\\left(\\begin{array}{ccc} \n",
    "k(\\mathbf{x}_1,\\mathbf{x}_1) & \\dots & k(\\mathbf{x}_1,\\mathbf{x}_n)\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "k(\\mathbf{x}_n,\\mathbf{x}_1) & \\dots & k(\\mathbf{x}_n,\\mathbf{x}_n)\n",
    "\\end{array}\\right)\n",
    "$$ \n",
    "by calling the kernel objected that is passed through the initialization of the KernelPerceptron. \n",
    "\n",
    "We then set the alpha values using one outer iteration up to the number of iterations T and one inner iteration up the number of samples. We then check if the function defined below \n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\text{sign}\\left(\\sum_{i=1}^{n} \\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{x})\\right)\n",
    "$$\n",
    "\n",
    "is not equal to y[i]. If this is the case, we add 1 to alpha[i].\n",
    "\n",
    "`project(self, X)`: In this method, we are creating a vector that stores the following result\n",
    "\n",
    "$$\n",
    "\\text{projection} = \\left(\\sum_{i=1}^{n} \\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{x})\\right)\n",
    "$$\n",
    "\n",
    "In the code, we achieve this using two iterations, the outer iteration being up to the number of rows in X and the inner iteration being up the number of rows in Xtrain. In each inner iteration we then compute the following \n",
    "\n",
    "$$\n",
    "\\alpha_i y_i k(\\mathbf{x}_i, \\mathbf{x})\n",
    "$$\n",
    "\n",
    "and add the result to projection[i].\n",
    "\n",
    "`predict(self, X)`: In this method, we call the project method we have just described above with X. We then take the resulting projection vector and iterate through each element. For each element, we check if the projection of that element is greater than or equal to 0 and if so set the prediction for this element in our y_hat vector to be 1. Otherwise, we set the prediction for this element in our y_hat vector to be -1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create three Kernel functions for a `PolynomialKernel`, `GaussianKernel`, and `LaplaceKernel`. Each KernelFunction and their methods are defined below.\n",
    "\n",
    "`PolynomialKernel`. The polynomial kernel function has two methods:\n",
    "\n",
    "`__init__(self, p=1)`: In this method, p is the degree of the polynomial and set to 1 when not specified\n",
    "\n",
    "`__call__(self, x, y)`: In this method, we implement the polynomial kernel function which is defined below\n",
    "\n",
    "$$\n",
    "k_{\\text{poly}}(\\mathbf{x},\\mathbf{x}', d) = (1+\\mathbf{x}^\\top \\mathbf{x}')^d\n",
    "$$\n",
    "\n",
    "In the code, the dot product of x transpose and y is added to 1, and then set to the pth exponent.\n",
    "\n",
    "`GaussianKernel`. The gaussian kernel function has two methods:\n",
    "\n",
    "`__init__(self, sigma=1)`: In this method, sigma is an important parameter in the RBF kernel function and is set to 5 as default. \n",
    "\n",
    "`__call__(self, x, y)`: In this method, we implement the gaussian kernel function which is defined below \n",
    "\n",
    "$$\n",
    "k_{\\text{RBF}}(\\mathbf{x},\\mathbf{x}', \\sigma) = \\exp\\left(-\\frac{\\lVert \\mathbf{x}-\\mathbf{x'} \\rVert^2_2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "In the code, np.linalg.norm is used to subtract x and y. The norm is then divided by the denominator of 2 * sigma ** 2. Then, np.exp of the negative result is returned.\n",
    "\n",
    "`LaplaceKernel`. The Laplace Kernel function has two methods: \n",
    "\n",
    "`__init__(self, sigma=1)`: In this method, sigma is an important parameter in the laplace kernel function and is set to 5 as default. \n",
    "\n",
    "`__call__(self, x, y)`: In this method, we implement the laplace kernel function which is defined below \n",
    "\n",
    "$$\n",
    "k_{\\text{laplace}}(\\mathbf{x},\\mathbf{x}', \\sigma) = \\exp\\left(-\\frac{\\lVert \\mathbf{x}-\\mathbf{x'} \\rVert _1}{\\sigma}\\right)\n",
    "$$\n",
    "\n",
    "In the code, np.linalg.norm with ord=1 is used to subtract x and y. Then the norm is divided by sigma, and then np.exp of the negative result is returned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Perceptron Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is the data linearly separable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will know check if the given data is linearly separable by calling the KernelPerceptron class with the PolynomialKernel function where p=1. We train the KernelPerceptron using Xtrain and ytrain and predicton on Xtest.\n",
    "\n",
    "Our model achieved an accuracy of :\n",
    "- Accuracy: 47.50%\n",
    "\n",
    "We then plot the decision boundary on the dataset as seen below\n",
    "\n",
    "![Is Lin Sep?]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More powerful kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform regression with the Polynomial Kernel, Gaussian Kernel, and Laplace Kernel functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We called the Kernel Perceptron class with the Polynomial Kernel function and set p = 2. The accuracy was 92.50% and the decision boundary is below:\n",
    "\n",
    "![Poly p 2]()\n",
    "\n",
    "We called the Kernel Perceptron class with the Polynomial Kernel function and set p = 3. The accuracy was 97.50% and the decision boundary is below:\n",
    "\n",
    "![Poly p 3]()\n",
    "\n",
    "We called the Kernel Perceptron class with the Polynomial Kernel function and set p = 4. The accuracy was 97.50% and the decision boundary is below:\n",
    "\n",
    "![Poly p 4]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Kernel**\n",
    "\n",
    "We called the Kernel Perceptron class with the Gaussian Kernel function and set sigma = 1. The accuracy was 97.50% and the decision boundary is below:\n",
    "\n",
    "![Gaus s 1]()\n",
    "\n",
    "We called the Kernel Perceptron class with the Gaussian Kernel function and set sigma = 3. The accuracy was 97.50% and the decision boundary is below:\n",
    "\n",
    "\n",
    "![Gaus s 3]()\n",
    "\n",
    "We called the Kernel Perceptron class with the Gaussian Kernel function and set sigma = 5. The accuracy was 97.50% and the decision boundary is below:\n",
    "\n",
    "\n",
    "![Gaus s 5]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Laplace Kernel**\n",
    "\n",
    "We called the Kernel Perceptron class with the Laplace Kernel function and set sigma = 0.001. The accuracy was 97.50% and the decision boundary is below:\n",
    "\n",
    "![Lap s 0.001]()\n",
    "\n",
    "We called the Kernel Perceptron class with the Laplace Kernel function and set sigma = 0.0015. The accuracy was 97.50% and the decision boundary is below:\n",
    "\n",
    "\n",
    "![Lap s 0.0015]()\n",
    "\n",
    "We called the Kernel Perceptron class with the Laplace Kernel function and set sigma = 0.01. The accuracy was 97.50% and the decision boundary is below:\n",
    "\n",
    "![Lap s 0.01]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2B: Real-World Data Analysis: Seoul Bike Rental Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will analyze the SeoulBikeData.csv dataset, which provides information about bike rentals in Seoul. The dataset includes:\n",
    "- **6 Features**: Weather-related conditions like temperature, humidity, and wind speed.\n",
    "- **1 Time Feature**: Hour of the day.\n",
    "- **Target**: The number of rented bikes, with the objective of predicting whether `Rented Bike Count > 500`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "1. **Load and Explore the Dataset**:\n",
    "   - Load the `SeoulBikeData.csv` file using `pandas`.\n",
    "   - Display descriptive statistics and visualize feature distributions (e.g., histograms, pair plots).\n",
    "\n",
    "\n",
    "In this step, we loaded the SeoulBikeData.csv file using pandas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

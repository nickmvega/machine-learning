{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 Report - Nick Vega\n",
    "\n",
    "In this project, we will construct a linear regression model, and apply it to a synthetic task as well as a real-world house value prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1A : Linear Regression\n",
    "\n",
    "In this task, my group implemented linear regression with gradient descent optimization from scratch. \n",
    "\n",
    "The dataset used was a synthetic dataset that was provided. \n",
    "\n",
    "In this task, my group learned how to\n",
    "- implement linear regression from scratch (no ML libraries)\n",
    "- implement loss functions and their gradient\n",
    "- implement and understand gradient descent optimization\n",
    "- implement visualizations and analyze model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implemented linear regression with gradient descent optimization from scratch, we were provided with a given problem setup that included \n",
    "- necessary dependencies\n",
    "- utility functions (data generation and visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the following generated data that is provided and used in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Generated Data](/workspaces/regression-and-model-selection/images/generated_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A.1: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK** \n",
    "\n",
    "In linear regression, we must find coefficents $\\hat{\\mathbf{w}}$ such that the residual between $\\hat{y} = \\hat{\\mathbf{w}}^\\top \\tilde{\\mathbf{x}}$, and $y$ is minimized.\n",
    "\n",
    "To find the optimal coefficents $\\hat{\\mathbf{w}}$ that minimizes the residuals, we will need to implement gradient descent optimization. However, before we implement gradient descent optimization, we must first implement a loss function that can calculate the gradient of the empirical risk function with respect to w and the regularized loss of the empirical risk during each iteration of gradient descent.\n",
    "\n",
    "In order to do this, we start with a ridge regression risk function, defined as \n",
    "\n",
    "$$ R({\\mathbf{w}}) = \\mathbb{E}[(y-{\\mathbf{w}}^\\top \\mathbf{x})^2)] +  \\lambda \\mathbf{w}^\\top \\mathbf{w}$$\n",
    "\n",
    "The ridge regression risk function will need to be approximated by the empirical risk. The result is:\n",
    "\n",
    "$$\\hat{R}_{\\text{ridge}}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\mathbf{w}^\\top \\mathbf{x}_i\\right)^2 + \\lambda \\mathbf{w}^\\top \\mathbf{w}$$\n",
    "\n",
    "In this task, we were instructued to construct a customized function that would return the empirical risk and its gradient at parameter w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION**\n",
    "\n",
    "We are tasked with constructed a customized function that will return the empirical risk and its gradient at parameter w.\n",
    "\n",
    "We defined a function named lossFunction that took a numpy array of w, X, and y, and a regularization paramter lambda and returned the computed ridge regression risk function and its gradient.\n",
    "\n",
    "In the function, the code initializes the regLoss to 0 and creates a gradient vector that is filled with zeros of the shape of w.\n",
    "\n",
    "Then, the code computes \n",
    "\n",
    "$$\\hat{R}_{\\text{ridge}}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\mathbf{w}^\\top \\mathbf{x}_i\\right)^2 + \\lambda \\mathbf{w}^\\top \\mathbf{w}$$\n",
    "\n",
    "where n is the number of rows in X. \n",
    "\n",
    "The gradient, the deraritive of the empirical risk at parameter w, is then calculated using the following formula as the code:\n",
    "\n",
    "$$\\nabla \\hat{R}_{\\text{ridge}}(\\mathbf{w}) = -\\frac{2}{n}\\mathbf{X}^\\top(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + 2\\lambda\\mathbf{w}$$\n",
    "\n",
    "where n is the number of rows in X.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPLEMENTATION** \n",
    "\n",
    "Given the generated data, the loss function is then used to compute and display the initial loss and gradient for regularized linear regression. \n",
    "\n",
    "The result is the following: \n",
    "\n",
    "Loss at initial w (zeros): [[8.12449231]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A.2: Training Your Ridge Regressor: Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "As mentioned in Task 1A.1,  we must find coefficents $\\hat{\\mathbf{w}}$ such that the residual between $\\hat{y} = \\hat{\\mathbf{w}}^\\top \\tilde{\\mathbf{x}}$, and $y$ is minimized. In order to find the coefficients $\\hat{\\mathbf{w}}$ such that the residual is minimized, we will implement gradient descent optimization from scratch.\n",
    "\n",
    "In Task 1A.1, we constructed a loss function that would compute the empirical risk and its gradient at paramter w. \n",
    "\n",
    "Now, in the current task, we will train our ridge regression by implementing gradient descent optimization from scratch. \n",
    "\n",
    "The parameters $\\hat{\\mathbf{w}}$ can be updated via a gradient descent rule: \n",
    "\n",
    "$$ \\hat{\\mathbf{w}}_{t+1} \\gets \\hat{\\mathbf{w}}_t - \\eta_t \\left.\\frac{\\partial \\hat{R}}{\\partial \\mathbf{w}} \\right|_{\\mathbf{w}=\\hat{\\mathbf{w}}_t},$$\n",
    "\n",
    "where $\\eta_t$ is a parameter of the algorithm, $t$ is the iteration index, and $\\frac{\\partial \\hat{R}}{\\partial \\mathbf{w}}$ is the gradient of the empirical risk function w.r.t. $\\mathbf{w}$.\n",
    "\n",
    "We will keep $\\eta_t$ constant. The computational complexity of gradient descent is $O(n_{\\text{iter}} \\cdot  n d)$. \n",
    "\n",
    "In this task, we will write a customized gradient descent optimization function which will return an array of empirical risk values, one for each iteration, as well as the final output of the model paramter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION**\n",
    "\n",
    "We are tasked with writing a customized gradient descent optimization function which will return an array of empirical risk values, one for each iteration, as well as the final output of the model paramter.\n",
    "\n",
    "We defined a function called gradient descent which took numpy arrays for X, y, and w, and a float for eta, lambda, and tolerance, and a integer for a maximum number of iterations conducted. The function returns a tuple of a numpy array and a list with the final coefficients and loss history. \n",
    "\n",
    "`tolerance` specifies the stopping condition: The gradient descent algorithm terminates the observed loss values converges (i.e. two consective losses differ by at most `tolerance`). \n",
    "\n",
    "In the function, the code initalizes an empty Loss_history list and a prev_loss of infinity which signifies the loss in the preivous iteration of gradient descent. \n",
    "\n",
    "To initialize the gradient descent, the function calls the lossFunction function we made in task 1A.1 and appends the loss to the list.\n",
    "\n",
    "The function then enters a while loop that terminates when the current loss in the current iteration minus the previous loss from the previous iteration is greater than the tolerance. \n",
    "\n",
    "In the loop, for each iteration, the gradient descent rule\n",
    "\n",
    "$$ \\hat{\\mathbf{w}}_{t+1} \\gets \\hat{\\mathbf{w}}_t - \\eta_t \\left.\\frac{\\partial \\hat{R}}{\\partial \\mathbf{w}} \\right|_{\\mathbf{w}=\\hat{\\mathbf{w}}_t},$$\n",
    "\n",
    "is conducted with code with the gradient coming from the lossFunction function\n",
    "\n",
    "The prev_loss is then set to the current loss.\n",
    "\n",
    "The lossFunction function from task 1A.1 is then called, and the loss is appended to the Loss_history list. \n",
    "\n",
    "The function will also terminate if the a specified number of maximum iterations is reached. \n",
    "\n",
    "Once terminated, the function then returns the current w and Loss_history list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPLEMENTATION** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent function we have constructed is called using the generated data for X , y , and w (X=X_train, y=y_train, and w = initial_w)\n",
    "\n",
    "The eta = 0.1, the tolerance = 1e-6, and the max_iter = 1e4\n",
    "\n",
    "The result of performing gradient descent using the customized gradient descent function is: \n",
    "\n",
    "The regularized w using ridge regression:\n",
    " [[1.8996459 ]\n",
    " [0.64674945]]\n",
    "\n",
    "![Loss Function Using GD](/workspaces/regression-and-model-selection/images/loss_function_gd.png)\n",
    "\n",
    "From the graph, the loss(w) is minimized after ~10-15 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A.3: Test Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "In this task, we will evaulate the model we have constructed by plotting the predicted values on the test data, together with the training points. The code was implemented for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPLEMENTATION**\n",
    "\n",
    "![Ridge Regression](/workspaces/regression-and-model-selection/images/ridge_regression.png)\n",
    "\n",
    "From the graph, the ridge regression fit on the generated data given seems to minimize the residuals effectively. The process for genearting the ridge regression fit included constructing a customized loss function to compute the empirical risk and its gradient, constructing a customized gradient descent optimization function to compute the optimal parameter w using the gradient descent rule that called the customized loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A.4: True Risk [Bonus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will compute the expected error (true risk) of our model and then answer questions related to model selection based on our observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1A.4.1**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will compute and compare the expected error and estimated error of the trained model. \n",
    "\n",
    "The true risk for our linear regression model can be directly computed since we know:\n",
    "- Input $\\mathbf{x} = [x_0, 1]^\\top$ is sampled from standard normal distribution where $x_0 \\sim \\mathcal{N}(0,1)$\n",
    "- True parameter is $\\mathbf{w}_\\text{true} = [2, -2]^T$\n",
    "- Noise follows $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ where $\\sigma = 0.6$\n",
    "\n",
    "For a learned parameter $\\hat{\\mathbf{w}}$, the true error given MSE as the loss function is:\n",
    "\n",
    "\\begin{align}\n",
    "E(\\hat{\\mathbf{w}}) &= \\mathbb{E}_\\mathbf{x}[(\\mathbf{w}_\\text{true}^\\top \\mathbf{x} + \\epsilon - \\hat{\\mathbf{w}}^\\top \\mathbf{x})^2] \\\\\n",
    "\n",
    "&= \\mathbb{E}_\\mathbf{x}[(\\mathbf{w}_\\text{true}^\\top \\mathbf{x} - \\hat{\\mathbf{w}}^\\top \\mathbf{x})^2] + \\mathbb{E}[\\epsilon^2] \\\\\n",
    "\n",
    "&= (\\mathbf{w}_\\text{true} - \\hat{\\mathbf{w}})^\\top \\mathbb{E}[\\mathbf{x}\\mathbf{x}^\\top](\\mathbf{w}_\\text{true} - \\hat{\\mathbf{w}}) + \\sigma^2 \\\\\n",
    "\n",
    "&= (\\mathbf{w}_\\text{true} - \\hat{\\mathbf{w}})^\\top I(\\mathbf{w}_\\text{true} - \\hat{\\mathbf{w}}) + \\sigma^2 \\\\\n",
    "\n",
    "&= \\|\\mathbf{w}_\\text{true} - \\hat{\\mathbf{w}}\\|_2^2 + \\sigma^2\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where we used:\n",
    "- $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^T] = I$ (for $x_0 \\sim \\mathcal{N}(0,1)$)\n",
    "- $\\mathbb{E}[\\epsilon^2] = \\sigma^2$\n",
    "- Independence of $\\mathbf{x}$ and $\\epsilon$\n",
    "\n",
    "**K-fold Cross-validation:**\n",
    "\n",
    "We can compare this theoretical expected error with estimates from $k$-fold cross-validation:\n",
    "\n",
    "$$\\hat{E}(\\hat{\\mathbf{w}}) = \\frac{1}{k} \\sum_{i=1}^k \\left[ \\frac{1}{|V_i|} \\sum_{(\\mathbf{x},y)\\in V_i} (y - \\hat{\\mathbf{w}}_i^\\top \\mathbf{x})^2 \\right]$$\n",
    "\n",
    "where\n",
    "- $k$ is the number of folds\n",
    "- $V_i$ is the validation set ($i$-th fold)\n",
    "- $\\hat{\\mathbf{w}}_i$ is the model trained on all folds except $V_i$\n",
    "- $|V_i|$ is the size of the validation fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION 1A.4.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is calculate the true expected error. We are given a function called true_gen_error that takes in a numpy array of w (learned polynomial coefficients) and w_true (true polynomial coefficients) and sigma (the standard deviation of Gaussian noise) and returns teh true expected error or true risk. \n",
    "\n",
    "The function is implemented as follows. An error of 0 is initalized. \n",
    "\n",
    "Then, the true risk is computed using the following formula: \n",
    "\n",
    "$$||\\mathbf{w}_\\text{true} - \\hat{\\mathbf{w}}||_2^2 + \\sigma^2$$\n",
    "\n",
    "The formula is computed using functions from numpy such as np.subtract(w_true, w), and then takes the 2-norm using np.linalg.vector_num and sqaures the result. the result is then added to sigma squared. \n",
    "\n",
    "The estimated expected erorr using k-fold cross-validation is implemented for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPLEMENTATION 1A.4.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned parameter $\\hat{\\mathbf{w}}$ is computed using the customzied gradient descent function we constructed previously. The true risk is then computed using the true_gen_error function we implemented with w being the learned parameter $\\hat{\\mathbf{w}}$ from gradient descent, w_true being a np.array of 2, 0.6, and sigma being 0.6. The 5-fold CV estimate is computed using the function they gave us.\n",
    "\n",
    "The results are the following:\n",
    "\n",
    "True expected error: 0.4319\n",
    "\n",
    "5-fold CV estimate: 0.4056 (±0.1941)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1A.4.2 Convergence of Expected Error**\n",
    "\n",
    "This task focuses on the principal question : how fast the rate of convergence of the learning process. The role of observaition noise shapes the convergence rate of expected error in empirical study. As noise increases, the error bounds are inflated and slow down the overall convergence. \n",
    "\n",
    "The task in this subsection is to implement the naive linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION 1A.4.2 Convergence of Expected Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task in this subsection is to implement naive linear regression. The function fit_naive_linear_regression, which takes numpy arrays X and y and returns a numpy array w that fits the linear regression without regularization.\n",
    "\n",
    "w was caluclated corresponding to the following formula below:\n",
    "\n",
    "$$\\hat{\\mathbf{w}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPLEMENTATION 1A.4.2 Convergence of Expected Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following graphs below depict the Ridge regression : True Expected Error, the Naive Regression True Expected Error, the Ridge Regression : CV Est. Error, the Naive Regression CV Est Error, the Ridge Regression Error Ratio, and the Naive Regression Error Ratio. \n",
    "\n",
    "![Generated Data Bonus](/workspaces/regression-and-model-selection/images/bonus_data.png)\n",
    "\n",
    "![Bonus All Graphs](/workspaces/regression-and-model-selection/images/bonus_multiple_graphs.png)\n",
    "\n",
    "We will address the following questions:\n",
    "\n",
    "- How does observation noise affect generalization performance?\n",
    "- Under what conditions is ridge regression meaningful, and when does naïve linear regression lead to overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1B: Model Selection for House Value Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will use the California housing dataset from the scikit-learn package to predict the house values in California districts given some summary stats about them based on the 1990 census data.\n",
    "\n",
    "The dataset has 8 features: longitudes, latitudes, housing median age, total rooms, total bedrooms, population, households, median income, and median house value. The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars ($\\$100,000$). We split the dataset as 80\\% for training data and 20\\% for testing data. \n",
    "\n",
    "We were given scripts that loaded the california housing dataset into a pandas dataframe. The dataframe is below:\n",
    "\n",
    "![California Housing Dataset](/workspaces/regression-and-model-selection/images/california_housing_dataset.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1B.1: Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were given a script that generated the training and test data.\n",
    "\n",
    "Task 1B.1 includes two sub sections, *Training and Evaluation* and *Model Selection via k-fold Cross Validation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK Training and Evaluation**\n",
    "\n",
    "In this subsection, we are tasked with writing a customized function to fit the ridge regression model on the training data and calculate the MSE on the test set.\n",
    "\n",
    "The function should choose $\\lambda$ from $\\{10^{-10}, 10^{-6}, 10^{-4}, 10^{-2}, 10^{-1}, 1, 10, 20, 50, 100\\}$, compute the estimate $\\hat{\\mathbf{y}}$ for different values $\\lambda$, and plot the test MSE as a function of $\\lambda$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION Training and Evaluation**\n",
    "\n",
    "In this subsection, we are tasked with writing a customized function to fit the ridge regression model on the training data and calculate the MSE on the test set.\n",
    "\n",
    "The function can be described as follow. The customized function is named train_and_eval and takes numpy arrays of X_train, y_train, X_eval, and y_eval, a lambda value and returns the mean squared error. \n",
    "\n",
    "Initially, the mse is set to 0, eta is set to 0.001, and tolerance is set to 1e-4. Then, the customized gradient descent function is called using X_train, y_train, initial_w, eta, a lambda value, and tolerance. The optimized weight computed from the gradient descent function is used to make predictions on the evaluation set as follows:\n",
    "\n",
    "$$\\hat{y} = \\mathbf{X}\\mathbf{w}$$\n",
    "\n",
    "where X is X_eval and w is the optimized weight from gradient descent.\n",
    "\n",
    "The mean squared error is then calculated using the following formula:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "The function then returns the calculated mse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPLEMENTATION Training and Evaluation** \n",
    "\n",
    "The customized train_and_eval function is called\n",
    "for each lambda value in the list $\\{10^{-10}, 10^{-6}, 10^{-4}, 10^{-2}, 10^{-1}, 1, 10, 20, 50, 100\\}$\n",
    "and the given X_train , y_train, X_test, and y_test. The mse from each lambda is then put into an array.\n",
    "\n",
    "The resulting array from calculating the mse from each lambda is: \n",
    "\n",
    "[[4.93873158e+00 1.00000000e-10],\n",
    "\n",
    "[4.93873173e+00 1.00000000e-06],\n",
    "\n",
    "[4.93874647e+00 1.00000000e-04],\n",
    "\n",
    "[4.94171940e+00 1.00000000e-02],\n",
    "\n",
    "[4.96865133e+00 1.00000000e-01],\n",
    "\n",
    "[5.17883497e+00 1.00000000e+00],\n",
    "\n",
    "[5.52571622e+00 1.00000000e+01],\n",
    "\n",
    "[5.57193418e+00 2.00000000e+01],\n",
    "\n",
    "[5.60276505e+00 5.00000000e+01],\n",
    "\n",
    "[5.61368148e+00 1.00000000e+02]]\n",
    "\n",
    "The array of mse is then visualized as follows:\n",
    "\n",
    "![Test MSE](/workspaces/regression-and-model-selection/images/test_mse.png)\n",
    "\n",
    "We will know answer the folowing question:\n",
    "\n",
    "- Suppose you found above that the optimal $\\lambda$ was very close to 0. Does this necessarily mean that Ridge regression is not helpful for this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK Model Selection via k-fold Cross Validation**\n",
    "\n",
    "In this subsection, we are tasked with implementing 10-fold cross validation on the training set to select lambda. We will perform k-fold cross validation on X_train and y_train, and return the mean square error on the test portion of (X_train, y_train) across the k-folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION Model Selection via k-fold Cross Validation**\n",
    "\n",
    "In this subsection, we are tasked with implementing 10-fold cross validation on the training set to select lambda. \n",
    "\n",
    "The function can be described as follow. The customized function is named cross_validation and takes numpy arrays of X_train, y_train, and lambda value, and a k (k=10) to determine the number of folds.\n",
    "\n",
    "The function initializes an empty mse_list. Then, the fold size is determined by the number of rows in X_train divided (floor) by k which divides the X_train data in k parts.\n",
    "\n",
    "Then, the function loops for every fold up to k folds, and in each loops does the following:\n",
    "- constructs the test indices for the current fold\n",
    "- constructs the train indices by using np.concatenate for data before the fold and after the fold\n",
    "- slices X_train and y_train using the test and train indcies\n",
    "- computes the mse using the train_and_eval function that was constructed in the previous section\n",
    "- then appends the mse to the mse list\n",
    "\n",
    "The function then returns the mean of the mse_list after the loop has concluded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPLEMENTATION Model Selection via k-fold Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function cross_validation is then called to find the best lambda from a given list of lambdas. For each lambda in the list, the cross_validation function is called and the resulting mse value is stored. The best mse value is found within the loop for each lambda by comparing the current mse to the best mse and replacing the best mse if the current mse is less than the best mse. \n",
    "\n",
    "The results are the following:\n",
    "\n",
    "cross validation with lambda:  1e-10\n",
    "\n",
    "cross validation with lambda:  1e-06\n",
    "\n",
    "cross validation with lambda:  0.0001\n",
    "\n",
    "cross validation with lambda:  0.01\n",
    "\n",
    "cross validation with lambda:  0.1\n",
    "\n",
    "cross validation with lambda:  1\n",
    "\n",
    "cross validation with lambda:  10\n",
    "\n",
    "cross validation with lambda:  20\n",
    "\n",
    "cross validation with lambda:  50\n",
    "\n",
    "cross validation with lambda:  100\n",
    "\n",
    "Best lambda:  1\n",
    "\n",
    "We will know plot and compare the **(average) cross validation MSE** on the hold-out folds (i.e., test portion in each of the $k$ folds) with the **test MSE** which is computed on the test set (X_test, y_test). And see how we get to finding the ``best'' $\\lambda$.\n",
    "\n",
    "We will know answer the following questions:\n",
    "- Compare the model selection strategy used in vanilla ridge regression (i.e., on a single train/test split) and in $k$-fold cross validation. What are the pros and cons of each approach.\n",
    "- Suppose the best $\\lambda$ chosen by 10-fold cross-validation is significantly different from the best $\\lambda$ you found using vanilla Ridge Regression. Which value would you trust more, and why? What are some potential reasons for this discrepancy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1B.2 Polynomial Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "In this task, we will perform a polynomial regression on the California housing dataset, and use cross-validation to determine the optimal degree of polynomial. \n",
    "\n",
    "Following the previous notation, we will consider the mean square risk function, defined as $$ R({\\mathbf{w}}) = \\mathbb{E}[(y-{\\mathbf{w}}^\\top \\mathbf{x})^2)].$$ For a polynomial regression of degree $D$, this becomes $$ R({\\mathbf{w}}) = \\mathbb{E}[(y-\\sum_{d = 1}^{D}w_d x^d)^2)] = \\mathbb{E}[(y-{\\mathbf{w}}^\\top \\mathbf{x})^2],$$ \n",
    "\n",
    "The risk is approximated by the *empirical risk* as: $$\\hat{R}_{\\text{MSE}}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\sum_{d = 0}^{D}w_d x_i^d\\right)^2.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will implemnet 4 functions.\n",
    "- lossFunctionMSE\n",
    "- gradientDescentMSE\n",
    "- train_and_eval_dim\n",
    "- cross_validation_dim\n",
    "\n",
    "In all of this functions, the lambda parameter has beem removed if it had one originally. \n",
    "\n",
    "**lossFunctionMSE** \n",
    "\n",
    "In this function, we take in an numpy array of w, X, and y and return the loss function and gradient for polynomial regression. \n",
    "\n",
    "The function initalizes the loss to 0 and gradient corresponding the shape of w. \n",
    "\n",
    "We will start by explaining how the mse loss is computed. The loss mse is calculated with the following formula: $$\\hat{R}_{\\text{MSE}}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\sum_{d = 0}^{D}w_d x_i^d\\right)^2.$$\n",
    "A outer loop is a loop up to the number of rows in X. Within the outer loop, an inner loop up to the number of degrees computes the following \n",
    "\n",
    "$$sum_d = \\sum_{d = 0}^{D}w_d x_i^d$$\n",
    "\n",
    "Then, \n",
    "\n",
    "$$sum_n = y_i - sum_d$$\n",
    "\n",
    "Then, $$sum_n ** 2$$ is added to the loss.\n",
    "\n",
    "Once the outer loop terminates, the loss is then divided by n and returned. \n",
    "\n",
    "We will know explain how the gradient is computed. The gradient is computed using the following formula:\n",
    "\n",
    "$$\\nabla \\hat{R}_{\\text{MSE}}(\\mathbf{w}) = -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\sum_{d=0}^D w_d x_{i,d}) x_{i,d}$$\n",
    "\n",
    "The gradient is calculated while the loss is being calculated. The outer loop and inner loop stays the same. The only addition is that once $$sum_n$$ is calculated. There is another inner loop up to the number of degrees and computes the following.\n",
    "\n",
    "$$grad[d] = grad[d] + (sum_n * X[i,d])$$\n",
    "\n",
    "Once the outer loop terminates, the calculated gradient (grad) is mulitpled by negative 2 and divided by n and returned. \n",
    "\n",
    "**gradientDescentMSE** \n",
    "\n",
    "This function is essentially equivalent to function gradientDescent that was constructed and described in task 1A.2 : Training your Ridge Regression: Gradient Descent. \n",
    "\n",
    "The only difference between the functions is the name and in gradientDescentMSE instead of calling lossFunction, it calls lossFunctionMSE and uses the loss computed from that function instead. The output is the same being the optimized w and a loss history. \n",
    "\n",
    "**train_and_eval_dim**\n",
    "\n",
    "This function is also essentially equivalent to the function train_and_eval that was constructed and described in task 1B.1 : Training and Evaluation.\n",
    "\n",
    "The main difference is that the function takes on an extra parameter called dim, which specifies the dimension. \n",
    "\n",
    "The difference between these two functions is that the given X_train and X_eval are transformed based on the given dim. X_train turns into Xtrain and X_eval turns into Xeval. The transformation is polynominal.\n",
    "\n",
    "Another difference is that after X_train and X_eval have been transformed, the eta = 0.001 and tolerance = 1e-4 are the same, however instead of calling gradientDescent, the function calls gradientDescentMSE with Xtrain, y_train, initial_w (which has been transformed based on the dim), eta, and tolerance. \n",
    "\n",
    "The function then returns two MSE, one MSE based on y_eval and one MSE based on y_train. \n",
    "\n",
    "The following functions are used in the code:\n",
    "\n",
    "$$ y_{pred} = Xeval @ w$$\n",
    "\n",
    "$$ y_{pred_{train}} = Xtrain @ w$$\n",
    "\n",
    "$$MSE = mean ((y_{eval} - y_{pred})**2)$$\n",
    "\n",
    "$$MSE_{train} = mean ((y_{train} - y_{pred_{train}}) ** 2)$$\n",
    "\n",
    "MSE and MSE_train are then returned. \n",
    "\n",
    "**cross_validation_dim**\n",
    "\n",
    "This function is very similar to cross_validation that was constructed and described in task 1B.1 : Model Selection via k-fold Cross Validation.\n",
    "\n",
    "This function has an extra parameter dim that the original did not have. \n",
    "\n",
    "The only other difference is that within the main loop of the function, instead of calling train_and_eval, train_and_eval_dim is called instead with dim being an extra parameter. The result from train_and_eval_dim is the mse and mse_train that was described in the previous part. \n",
    "\n",
    "The function appends the mse and mse_train to two separate lists, then returns the mean of both lists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPLEMENTATION**\n",
    "\n",
    "A given degree list is iterated through calling cross_validation_dim with a given X_train, y_train, and the current degree. The result of cross_validation_dim is two mse values, the cv_mse and cv_mse_train. The graph below displays how these mse values are visualized for varying degrees.\n",
    "\n",
    "![Polynomial graph](/workspaces/regression-and-model-selection/images/polynomial.png)\n",
    "\n",
    "We will know answer the following questions:\n",
    "\n",
    "- How would you use this information to determine the degree of polynomial you will use to represent this dataset?\n",
    "\n",
    "Based on the graph, I would determine the degree of polynomial based on the mse value for the cv validation mse and and the complexity of the degree. Given that the difference of mse between degree 1 and degree 4 is about ~0.2, I would choose a degree of polynomial 1 given that it a degree that is not complex as a degree of 4 and still has a low mse value compared to a degree of 0. \n",
    "\n",
    "- Plot and analyze how the increasing degree of the polynomial change the training and validation loss. Do they change in different ways?\n",
    "\n",
    "The CV training mse has been plotted along with the CV validation mse. The mse value itself changes in different ways, being the CV training mse always has a lower mse values than the CV validation mse. The CV training mse has an initial mse of ~1.7 while the cv validation mse has an initial mse of ~1.85. They both drop by about ~0.8 mse at degree of polynomial 1. The validation mse does not increase as much as the training mse from 1 to 2. They both drop in mse from 2 to 3. Finally from 3 to 4, the validation mse drops while the training mse increases."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

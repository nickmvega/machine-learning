{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 Report - Nick Vega\n",
    "\n",
    "The project report should include a brief justification of your solution at a high-level, e.g., using any relevant explanations, equations, or pictures that help to explain your solution. You should also describe what your code does, e.g. using a couple of sentences per function to describe your code structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1A : Linear Regression\n",
    "\n",
    "In this task, my group implemented linear regression with gradient descent optimization from scratch. \n",
    "\n",
    "The dataset used was a synthetic dataset that was provided. \n",
    "\n",
    "In this task, my group learned how to\n",
    "- implement linear regression from scratch (no ML libraries)\n",
    "- implement loss functions and their gradient\n",
    "- implement and understand gradient descent optimization\n",
    "- implement visualizations and analyze model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implemented linear regression with gradient descent optimization from scratch, we were provided with a given problem setup that included \n",
    "- necessary dependencies\n",
    "- utility functions (data generation and visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the following generated data that is provided and used in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Generated Data](generated_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A.1: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK** \n",
    "\n",
    "In linear regression, we must find coefficents $\\hat{\\mathbf{w}}$ such that the residual between $\\hat{y} = \\hat{\\mathbf{w}}^\\top \\tilde{\\mathbf{x}}$, and $y$ is minimized.\n",
    "\n",
    "To find the optimal coefficents $\\hat{\\mathbf{w}}$ that minimizes the residuals, we will need to implement gradient descent optimization. However, before we implemnet gradient descent optimization, we must first implement a loss function that can calculate the gradient of the empirical risk function with respect to w and the regularized loss of the empirical risk during each iteration of gradient descent.\n",
    "\n",
    "In order to do this, we start with a ridge regression risk function, defined as \n",
    "\n",
    "$$ R({\\mathbf{w}}) = \\mathbb{E}[(y-{\\mathbf{w}}^\\top \\mathbf{x})^2)] +  \\lambda \\mathbf{w}^\\top \\mathbf{w}$$\n",
    "\n",
    "The ridge regression risk function will need to be approximated by the empirical risk. The result is:\n",
    "\n",
    "$$\\hat{R}_{\\text{ridge}}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\mathbf{w}^\\top \\mathbf{x}_i\\right)^2 + \\lambda \\mathbf{w}^\\top \\mathbf{w}$$\n",
    "\n",
    "In this task, we were instructued to construct a customized function that would return the empirical risk and its gradient at parameter w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION**\n",
    "\n",
    "We are tasked with constructed a customized function that will return the empirical risk and its gradient at parameter w.\n",
    "\n",
    "We defined a function named lossFunction that took a numpy array of w, X, and y, and a regularization paramter lambda and returned the computed ridge regression risk function and its gradient.\n",
    "\n",
    "In the function, the code initializes the regLoss to 0 and creates a gradient vector that is filled with zeros of the shape of w.\n",
    "\n",
    "Then, the code computes \n",
    "\n",
    "$$\\hat{R}_{\\text{ridge}}(\\mathbf{w}) = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\mathbf{w}^\\top \\mathbf{x}_i\\right)^2 + \\lambda \\mathbf{w}^\\top \\mathbf{w}$$\n",
    "\n",
    "where n is the number of rows in X. \n",
    "\n",
    "The gradient, the deraritive of the empirical risk at parameter w, is then calculated using the following formula as the code:\n",
    "\n",
    "$$\\nabla \\hat{R}_{\\text{ridge}}(\\mathbf{w}) = -\\frac{2}{n}\\mathbf{X}^\\top(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + 2\\lambda\\mathbf{w}$$\n",
    "\n",
    "where n is the number of rows in X.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPLEMENTATION** \n",
    "\n",
    "Given the generated data, the loss function is then used to compute and display the initial loss and gradient for regularized linear regression. \n",
    "\n",
    "The result is the following: \n",
    "\n",
    "Loss at initial w (zeros): [[8.12449231]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A.2: Training Your Ridge Regressor: Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "As mentioned in Task 1A.1,  we must find coefficents $\\hat{\\mathbf{w}}$ such that the residual between $\\hat{y} = \\hat{\\mathbf{w}}^\\top \\tilde{\\mathbf{x}}$, and $y$ is minimized. In order to find the coefficients $\\hat{\\mathbf{w}}$ such that the residual is minimized, we will implement gradient descent optimization from scratch.\n",
    "\n",
    "In Task 1A.1, we constructed a loss function that would compute the empirical risk and its gradient at paramter w. \n",
    "\n",
    "Now, in the current task, we will train our ridge regression by implementing gradient descent optimization from scratch. \n",
    "\n",
    "The parameters $\\hat{\\mathbf{w}}$ can be updated via a gradient descent rule: \n",
    "\n",
    "$$ \\hat{\\mathbf{w}}_{t+1} \\gets \\hat{\\mathbf{w}}_t - \\eta_t \\left.\\frac{\\partial \\hat{R}}{\\partial \\mathbf{w}} \\right|_{\\mathbf{w}=\\hat{\\mathbf{w}}_t},$$\n",
    "\n",
    "where $\\eta_t$ is a parameter of the algorithm, $t$ is the iteration index, and $\\frac{\\partial \\hat{R}}{\\partial \\mathbf{w}}$ is the gradient of the empirical risk function w.r.t. $\\mathbf{w}$.\n",
    "\n",
    "We will keep $\\eta_t$ constant. The computational complexity of gradient descent is $O(n_{\\text{iter}} \\cdot  n d)$. \n",
    "\n",
    "In this task, we will write a customized gradient descent optimization function which will return an array of empirical risk values, one for each iteration, as well as the final output of the model paramter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION**\n",
    "\n",
    "We are tasked with writing a customized gradient descent optimization function which will return an array of empirical risk values, one for each iteration, as well as the final output of the model paramter.\n",
    "\n",
    "We defined a function called gradient descent which took numpy arrays for X, y, and w, and a float for eta, lambda, and tolerance, and a integer for a maximum number of iterations conducted. The function returns a tuple of a numpy array and a list with the final coefficients and loss history. \n",
    "\n",
    "`tolerance` specifies the stopping condition: The gradient descent algorithm terminates the observed loss values converges (i.e. two consective losses differ by at most `tolerance`). \n",
    "\n",
    "In the function, the code initalizes an empty Loss_history list and a prev_loss of infinity which signifies the loss in the preivous iteration of gradient descent. \n",
    "\n",
    "To initialize the gradient descent, the function calls the lossFunction function we made in task 1A.1 and appends the loss to the list.\n",
    "\n",
    "The function then enters a while loop that terminates when the current loss in the current iteration minus the previous loss from the previous iteration is greater than the tolerance. \n",
    "\n",
    "In the loop, for each iteration, the gradient descent rule\n",
    "\n",
    "$$ \\hat{\\mathbf{w}}_{t+1} \\gets \\hat{\\mathbf{w}}_t - \\eta_t \\left.\\frac{\\partial \\hat{R}}{\\partial \\mathbf{w}} \\right|_{\\mathbf{w}=\\hat{\\mathbf{w}}_t},$$\n",
    "\n",
    "is conducted with code with the gradient coming from the lossFunction function\n",
    "\n",
    "The prev_loss is then set to the current loss.\n",
    "\n",
    "The lossFunction function from task 1A.1 is then called, and the loss is appended to the Loss_history list. \n",
    "\n",
    "The function will also terminate if the a specified number of maximum iterations is reached. \n",
    "\n",
    "Once terminated, the function then returns the current w and Loss_history list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPLEMENTATION** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent function we have constructed is called using the generated data for X , y , and w (X=X_train, y=y_train, and w = initial_w)\n",
    "\n",
    "The eta = 0.1, the tolerance = 1e-6, and the max_iter = 1e4\n",
    "\n",
    "The result of performing gradient descent using the customized gradient descent function is: \n",
    "\n",
    "The regularized w using ridge regression:\n",
    " [[1.8996459 ]\n",
    " [0.64674945]]\n",
    "\n",
    "![Loss Function Using GD](loss_function_gd.png)\n",
    "\n",
    "From the graph, the loss(w) is minimized after ~10-15 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A.3: Test Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A.4: True Risk [Bonus]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
